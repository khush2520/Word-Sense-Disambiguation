{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khush2520/Word-Sense-Disambiguation/blob/main/finallll_WSD_sem_cor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoX3C52FLSNN",
        "outputId": "65c59f5d-84dc-4f88-aaa3-dfeeccf61acf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "Vq7xbHHE4mWH"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "hXfXZDkO-RZr"
      },
      "outputs": [],
      "source": [
        "\n",
        "xml_file = 'drive/MyDrive/semcor.data.xml'\n",
        "gold_key = 'drive/MyDrive/semcor.gold.key.txt'\n",
        "test_xml_file = 'drive/MyDrive/ALL.data.xml'\n",
        "test_gold_key = 'drive/MyDrive/ALL.gold.key.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-nW7MDt_anY",
        "outputId": "1ad5ab80-c6a1-4030-c484-2cf06b20d92d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of the file is approximately 340.03 MB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "file_path = 'drive/MyDrive/dataset.json'  # Replace 'your_file_path_here' with the path to your file\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    # Get the size of the file in bytes\n",
        "    file_size_bytes = os.path.getsize(file_path)\n",
        "\n",
        "    # Convert bytes to megabytes\n",
        "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
        "\n",
        "    print(f\"The size of the file is approximately {file_size_mb:.2f} MB\")\n",
        "else:\n",
        "    print(\"File not found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "tWIaZLqZIbsU"
      },
      "outputs": [],
      "source": [
        "# def find_corpus(file_name):\n",
        "#     with open(file_name, 'r') as file:\n",
        "#         lines = file.read()\n",
        "#         a = (len(lines)//1)\n",
        "#         print(a)\n",
        "#         print(lines.count(\"/corpus\"))\n",
        "#         i = 0\n",
        "#         while \"<corpus\" in lines:\n",
        "#           i +=1\n",
        "#           index = lines.index(\"<corpus\")\n",
        "#           # print(lines[index:index+a//100])\n",
        "#           index2 = lines.index(\"</corpus\")\n",
        "\n",
        "\n",
        "\n",
        "#           # print(index,index2)\n",
        "#           corpus = lines[index:index2]\n",
        "#           print(index,index2,lines[index2-10:index2+10])\n",
        "#           lines = lines[0:index] + lines[index2+10:]\n",
        "#           with open(f'drive/MyDrive/temp{i}.xml', 'w') as file:\n",
        "\n",
        "#               file.write('<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\\n')\n",
        "#               file.write('<root>\\n')\n",
        "#               file.write(corpus)\n",
        "#               file.write('</corpus>\\n')\n",
        "#               file.write('</root>\\n')\n",
        "\n",
        "\n",
        "# find_corpus(xml_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "ISaxK3tUUjLn"
      },
      "outputs": [],
      "source": [
        "# print(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "x4d6nMlOnFI-"
      },
      "outputs": [],
      "source": [
        "# def divide_corpus(file_name):\n",
        "#     with open(file_name, 'r') as file:\n",
        "#         lines = file.read()\n",
        "#         a = (len(lines)//10)\n",
        "#         print(lines.count(\"/sentence\"))\n",
        "#         i = 0\n",
        "#         while \"<sentence\" in lines:\n",
        "#           i +=1\n",
        "#           index = lines.index(\"<sentence\")\n",
        "#           # print(lines[index:index+a//100])\n",
        "#           index2 = a + lines[a:].index(\"</sentence\")\n",
        "#           if len(lines) < 2* a:\n",
        "#               index2 = lines.rindex(\"</sentence\")\n",
        "\n",
        "\n",
        "\n",
        "#           # print(index,index2)\n",
        "#           corpus = lines[index:index2]\n",
        "#           print(index,index2,lines[index2-10:index2+12])\n",
        "#           lines = lines[0:index] + lines[index2+12:]\n",
        "#           with open(f'drive/MyDrive/tempdd{i}.xml', 'w') as file:\n",
        "\n",
        "#               file.write('<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\\n')\n",
        "#               file.write('<root>\\n')\n",
        "#               file.write(corpus)\n",
        "#               file.write('</sentence>\\n')\n",
        "#               file.write('</root>\\n')\n",
        "\n",
        "\n",
        "# divide_corpus(\"drive/MyDrive/temp2.xml\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "mZstcQ-ztTi7"
      },
      "outputs": [],
      "source": [
        "dataset= {}\n",
        "ins = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "7D6FdXGnui1E"
      },
      "outputs": [],
      "source": [
        "\n",
        "def process(file):\n",
        "    # root = ET.fromstring(corpus)\n",
        "    n = 0\n",
        "    global ins\n",
        "    tree = ET.parse(file)\n",
        "    root = tree.getroot()\n",
        "    # print(type(root))\n",
        "    for sentence in root.findall('.//sentence'):\n",
        "        # print(f\"Sentence ID: {sentence.attrib['id']}\")\n",
        "        sent = \"\"\n",
        "        n+=1\n",
        "        for element in sentence:\n",
        "                sent = sent + \" \" + element.text\n",
        "        for element in sentence:\n",
        "            if element.tag in ['instance']:\n",
        "                  ins += 1\n",
        "                  lemma = element.attrib.get('lemma')\n",
        "                  id = element.attrib.get('id')\n",
        "                  pos = element.attrib.get('pos')\n",
        "                  if lemma not in dataset:\n",
        "                    dataset[lemma] = []\n",
        "                  dataset[lemma].append([id,lemma, sent,pos,element.text])\n",
        "\n",
        "        # print(sent)\n",
        "\n",
        "    print(len(dataset))\n",
        "    return n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tN_4Xq_0B-uv",
        "outputId": "de129d5d-f71e-4058-ea3a-4c2174b352d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20399\n",
            "226036\n"
          ]
        }
      ],
      "source": [
        "process(xml_file)\n",
        "# for i in range(1,10):\n",
        "#     print(process(f'drive/MyDrive/tempdd{i}.xml'))\n",
        "print(ins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "WkKH4obaaavg"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('drive/MyDrive/semcordataset.json', 'w') as file:\n",
        "    json.dump(dataset, file, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "CgUDkc__aa6y"
      },
      "outputs": [],
      "source": [
        "with open('drive/MyDrive/semcordataset.json', 'r') as file:\n",
        "    dataset = json.load(file)\n",
        "i = 0\n",
        "for d in dataset:\n",
        "  i += 1\n",
        "  if i > 12:\n",
        "    break\n",
        "# print(d, dataset[d])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUVSq8Wg103J",
        "outputId": "4b31b158-6374-4545-d27b-244bc2c68338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "226036\n",
            "33316\n"
          ]
        }
      ],
      "source": [
        "lines = []\n",
        "with open(gold_key, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "Y_values = {}\n",
        "processed_data = []\n",
        "for line in lines:\n",
        "    words = line.strip().split()\n",
        "    processed_data.append(words[1])\n",
        "    Y_values[words[0]] = words[1]\n",
        "#     print(item)\n",
        "\n",
        "print(len(processed_data))\n",
        "print(len(set(processed_data)))\n",
        "# print(Y_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "X9SVPnwrAwjO"
      },
      "outputs": [],
      "source": [
        "\n",
        "for lemma in dataset:\n",
        "  for j in dataset[lemma]:\n",
        "    if (j[0] == None):\n",
        "          print(lemma,j)\n",
        "    y = Y_values[j[0]]\n",
        "    pos = y.index('%')\n",
        "    j.append(y[pos:pos+8])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFF3s28y1vxu",
        "outputId": "ab7b3b93-b526-4315-91a1-3290c04f3227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7253\n",
            "3612\n"
          ]
        }
      ],
      "source": [
        "file_path = test_gold_key\n",
        "# lines = []\n",
        "# with open(test_gold_key, 'r') as file:\n",
        "#     lines = file.readlines()\n",
        "# # print(lines[0:9])\n",
        "\n",
        "# test_Y_values = {}\n",
        "# test_processed_data = []\n",
        "# for line in lines:\n",
        "#     words = line.strip().split()\n",
        "#     test_processed_data.append(words[1])\n",
        "#     test_Y_values[words[0]] = words[1]\n",
        "#     # pos = words[0].find('%')\n",
        "\n",
        "#     # processed_data.append(words[0][:pos])\n",
        "\n",
        "# test_Y = test_processed_data\n",
        "# # print(test_Y_values)\n",
        "# print(len(test_processed_data))\n",
        "# print(len(set(test_processed_data)))\n",
        "# print(Y_values)lines = []\n",
        "with open(test_gold_key, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "test_Y_values = {}\n",
        "test_processed_data = []\n",
        "for line in lines:\n",
        "    words = line.strip().split()\n",
        "    pos = line.index(\" \")\n",
        "    line = line[pos:]\n",
        "    test_processed_data.append(line)\n",
        "    test_Y_values[words[0]] = line\n",
        "#     print(item)\n",
        "test_Y = test_processed_data\n",
        "\n",
        "print(len(test_processed_data))\n",
        "print(len(set(test_processed_data)))\n",
        "# print(Y_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KM5ajN4EgPv",
        "outputId": "a50e04a7-c019-447a-8408-5d1a1aa0d327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2659\n"
          ]
        }
      ],
      "source": [
        "\n",
        "test_tree = ET.parse(test_xml_file)\n",
        "test_root = test_tree.getroot()\n",
        "# print(type(root))\n",
        "test_dataset = {}\n",
        "for sentence in test_root.findall('.//sentence'):\n",
        "    # print(f\"Sentence ID: {sentence.attrib['id']}\")\n",
        "    sent = \"\"\n",
        "    for element in sentence:\n",
        "            sent = sent + \" \" + element.text\n",
        "    for element in sentence:\n",
        "            if element.tag in ['instance']:\n",
        "              lemma = element.attrib.get('lemma')\n",
        "              id = element.attrib.get('id')\n",
        "              pos = element.attrib.get('pos')\n",
        "              if lemma not in test_dataset:\n",
        "                test_dataset[lemma] = []\n",
        "              test_dataset[lemma].append([id,lemma, sent,pos,element.text])\n",
        "            # print(f\"{element.text} Lemma: {lemma}, POS: {pos}\")\n",
        "    # print(sent)\n",
        "\n",
        "print(len(test_dataset))\n",
        "# print(features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "VobYYLonGDRd"
      },
      "outputs": [],
      "source": [
        "for lemma in test_dataset:\n",
        "  for j in test_dataset[lemma]:\n",
        "    y = test_Y_values[j[0]]\n",
        "    pos = y.index('%')\n",
        "    j.append(y[pos:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "FWyFkQ_tRGfT"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "def print_all_senses(word):\n",
        "    synsets = wordnet.synsets(word)\n",
        "    if synsets:\n",
        "        print(f\"All senses of '{word}':\")\n",
        "        for synset in synsets:\n",
        "            print(f\"{synset.name()}:  Sense Key: {synset.lemmas()[-1].key()} {synset.definition()}\" )\n",
        "    else:\n",
        "        print(f\"No senses found for '{word}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "EPWEyepIVZgY"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "POS  = {'n':'NOUN', 'a': 'ADJ','s': 'ADJ', 'v': 'VERB', 'r':'ADV'}\n",
        "def add_dataset_from_wordnet(word):\n",
        "    word2 = word.replace('_',' ')\n",
        "    word2 = word2.replace('-',' ')\n",
        "    # print(\"word\", word)\n",
        "    synsets = wn.synsets(word)\n",
        "    if synsets:\n",
        "      for word_synset in synsets:\n",
        "        lemma = word_synset.lemmas()[0].name()\n",
        "        example_sentence = word_synset.examples()[0] if word_synset.examples() else word_synset.definition()\n",
        "\n",
        "        p = word_synset.pos()\n",
        "        pos = POS[p]\n",
        "\n",
        "        # Find the word form in the example sentence\n",
        "        word_form = None\n",
        "        if example_sentence is not None:\n",
        "          word_forms = example_sentence.split()\n",
        "          for wf in word_forms:\n",
        "            wf_synsets = wn.synsets(wf)\n",
        "            for wf_synset in wf_synsets:\n",
        "                wf_lemma = wf_synset.lemmas()[0].name()\n",
        "                # print(wf_l)\n",
        "                if lemma == wf_lemma:\n",
        "                  word_form = wf\n",
        "                  break;\n",
        "            if word_form:\n",
        "              break\n",
        "        for sense in word_synset.lemmas():\n",
        "          sense_key = sense.key()\n",
        "          posi = sense_key.index('%')\n",
        "          if sense_key.startswith(word+'%'):\n",
        "            posi = sense_key.index('%')\n",
        "            break\n",
        "\n",
        "        if word_form is None:\n",
        "          word_form = word\n",
        "        if word not in dataset:\n",
        "          dataset[word] = []\n",
        "        dataset[word].append([None, lemma, example_sentence, pos, word_form, sense_key[posi:]])\n",
        "        # print('hi', [lemma, example_sentence, pos, word_form, sense_key])\n",
        "    else:\n",
        "        print('hi',word, [None])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "Ww4allnlCXkc"
      },
      "outputs": [],
      "source": [
        "words = list(test_dataset.keys())\n",
        "\n",
        "for lemma in words:\n",
        "    add_dataset_from_wordnet(lemma)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('drive/MyDrive/final_dataset.json', 'w') as file:\n",
        "    json.dump(dataset, file, indent=2)\n"
      ],
      "metadata": {
        "id": "popvDOSP1SRp"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('drive/MyDrive/final_dataset.json', 'r') as file:\n",
        "    dataset = json.load(file)"
      ],
      "metadata": {
        "id": "zz-nfdTE1exQ"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rOM4Fx5l1eBj"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gm8rAu0D1eMb"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfjvnilZMyKJ",
        "outputId": "8d3c2d0b-19cb-45e4-9134-240006c77256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6554529160347442\n"
          ]
        }
      ],
      "source": [
        "#Navie Bayes\n",
        "words = list(test_dataset.keys())\n",
        "total_accuracy = 0\n",
        "lc = 0\n",
        "for lemma in words:\n",
        "    if lemma not in dataset:\n",
        "      add_dataset_from_wordnet(lemma)\n",
        "      # print(lemma)\n",
        "      continue\n",
        "\n",
        "    X_train = [row[1:-1] for row in dataset[lemma]]\n",
        "    y_train = [row[-1] for row in dataset[lemma]]\n",
        "\n",
        "\n",
        "    X_test = [row[1:-1] for row in test_dataset[lemma]]\n",
        "    y_test = [row[-1] for row in test_dataset[lemma]]\n",
        "    vectorizer = DictVectorizer(sparse=False)\n",
        "    X_train_vectorized = vectorizer.fit_transform([dict(zip(['lemma', 'sent', 'pos', 'element.text'], x)) for x in X_train])\n",
        "\n",
        "    # X_train_vectorized = vectorizer.fit_transform([dict(zip(['pos', 'element.text'], x)) for x in X_train])\n",
        "    naive_bayes_classifier = MultinomialNB()\n",
        "\n",
        "    naive_bayes_classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "    # X_test_vectorized = vectorizer.transform([dict(zip(['pos', 'element.text'], x)) for x in X_test])\n",
        "    X_test_vectorized = vectorizer.transform([dict(zip(['lemma', 'sent', 'pos', 'element.text'], x)) for x in X_test])\n",
        "    predictions = naive_bayes_classifier.predict(X_test_vectorized)\n",
        "    # accuracy = naive_bayes_classifier.score(X_test_vectorized, y_test)\n",
        "\n",
        "    if len(set(y_train)) < 2:\n",
        "        # if list(set(test_y))[0] != list(set(train_y))[0]:\n",
        "        #     print(lemma, test_y, list(set(train_y))[0])\n",
        "\n",
        "        lc += len(y_test)\n",
        "        total_accuracy += 1 * len(y_test)\n",
        "        continue\n",
        "    # print(\"Accuracy:\", accuracy)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for p, g in zip(predictions, y_test):\n",
        "      total += 1\n",
        "      if p in g:\n",
        "        correct += 1\n",
        "    accuracy = correct/total\n",
        "    lc+= len(X_test)\n",
        "    total_accuracy += accuracy * len(X_test)\n",
        "print(total_accuracy/lc)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5pW36JY7eb9U"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "MX0oA1EHT4uy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e214008-6927-4376-ef64-fe74ce5eb739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5891355301254654\n"
          ]
        }
      ],
      "source": [
        "#KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "words = list(test_dataset.keys())\n",
        "total_accuracy = 0\n",
        "lc = 0\n",
        "\n",
        "for lemma in words:\n",
        "    if lemma not in dataset:\n",
        "        print(lemma)\n",
        "        continue\n",
        "\n",
        "    X_train = [row[1:-1] for row in dataset[lemma]]\n",
        "    y_train = [row[-1] for row in dataset[lemma]]\n",
        "\n",
        "    X_test = [row[1:-1] for row in test_dataset[lemma]]\n",
        "    y_test = [row[-1] for row in test_dataset[lemma]]\n",
        "\n",
        "    if len(set(y_train)) < 2:\n",
        "        lc += len(y_test)\n",
        "        # total_accuracy += 1 * len(X_test)\n",
        "        continue\n",
        "\n",
        "    vectorizer = DictVectorizer(sparse=False)\n",
        "    X_train_vectorized = vectorizer.fit_transform([dict(zip(['lemma', 'sent', 'pos', 'element.text'], x)) for x in X_train])\n",
        "\n",
        "    knn_classifier = KNeighborsClassifier(n_neighbors=(max(2,len(y_train)//3)))  # You can adjust n_neighbors as needed\n",
        "\n",
        "    knn_classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "    X_test_vectorized = vectorizer.transform([dict(zip(['lemma', 'sent', 'pos', 'element.text'], x)) for x in X_test])\n",
        "\n",
        "    predictions = knn_classifier.predict(X_test_vectorized)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for p, g in zip(predictions, y_test):\n",
        "      total += 1\n",
        "      if p in g:\n",
        "        correct += 1\n",
        "    accuracy = correct/total\n",
        "\n",
        "\n",
        "\n",
        "    lc += len(X_test)\n",
        "    total_accuracy += accuracy * len(X_test)\n",
        "\n",
        "print(total_accuracy / lc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "JJZ6RnzloyJg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmYe4sNkIFDJ",
        "outputId": "16956c26-e9d8-419c-88a8-29f020e7c785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "art 0.0 \n",
            "\n",
            "peculiar 1.0 \n",
            "\n",
            "english 0.5 \n",
            "\n",
            "most 1.0 \n",
            "\n",
            "peculiarity 1.0 \n",
            "\n",
            "unintelligible 0.0 \n",
            "\n",
            "rest 1.0 \n",
            "\n",
            "world 0.0 \n",
            "\n",
            "tailor 1.0 \n",
            "\n",
            "england 1.0 \n",
            "\n",
            "scene 0.0 \n",
            "\n",
            "evoke 0.0 \n",
            "\n",
            "rural 1.0 \n",
            "\n",
            "lovely 1.0 \n",
            "\n",
            "ancient 1.0 \n",
            "\n",
            "stone 1.0 \n",
            "\n",
            "church 0.6666666666666666 \n",
            "\n",
            "stand 0.7142857142857143 \n",
            "\n",
            "field 0.375 \n",
            "\n",
            "sound 0.5 \n",
            "\n",
            "bell 1.0 \n",
            "\n",
            "cascade 0.0 \n",
            "\n",
            "tower 1.0 \n",
            "\n",
            "call 0.5555555555555556 \n",
            "\n",
            "faithful 0.0 \n",
            "\n",
            "evensong 0.0 \n",
            "\n",
            "parishioner 1.0 \n",
            "\n",
            "stop 0.2857142857142857 \n",
            "\n",
            "chat 1.0 \n",
            "\n",
            "door 0.4 \n",
            "\n",
            "member 0.7777777777777778 \n",
            "\n",
            "here 0.5714285714285714 \n",
            "\n",
            "always 1.0 \n",
            "\n",
            "man 0.9523809523809523 \n",
            "\n",
            "woman 1.0 \n",
            "\n",
            "pull 0.3333333333333333 \n",
            "\n",
            "rope 1.0 \n",
            "\n",
            "attach 0.0 \n",
            "\n",
            "same 0.5 \n",
            "\n",
            "first 0.7 \n",
            "\n",
            "also 1.0 \n",
            "\n",
            "discordant 0.0 \n",
            "\n",
            "modern 1.0 \n",
            "\n",
            "note 0.5 \n",
            "\n",
            "churchgoer 1.0 \n",
            "\n",
            "enjoy 1.0 \n",
            "\n",
            "peal 1.0 \n",
            "\n",
            "cool 1.0 \n",
            "\n",
            "autumn 1.0 \n",
            "\n",
            "evening 1.0 \n",
            "\n",
            "other 0.9696969696969697 \n",
            "\n",
            "britain 1.0 \n",
            "\n",
            "set 0.6 \n",
            "\n",
            "once 0.5 \n",
            "\n",
            "own 1.0 \n",
            "\n",
            "band 0.0 \n",
            "\n",
            "ringer 1.0 \n",
            "\n",
            "herald 1.0 \n",
            "\n",
            "sunday 1.0 \n",
            "\n",
            "morning 1.0 \n",
            "\n",
            "service 0.6 \n",
            "\n",
            "now 0.0 \n",
            "\n",
            "only 0.4117647058823529 \n",
            "\n",
            "local 0.875 \n",
            "\n",
            "remain 0.5 \n",
            "\n",
            "today 0.5 \n",
            "\n",
            "live 0.5714285714285714 \n",
            "\n",
            "elsewhere 1.0 \n",
            "\n",
            "belong_to 1.0 \n",
            "\n",
            "group 1.0 \n",
            "\n",
            "include 1.0 \n",
            "\n",
            "octogenarian 1.0 \n",
            "\n",
            "youngster 1.0 \n",
            "\n",
            "training 1.0 \n",
            "\n",
            "drive 0.2 \n",
            "\n",
            "effort 1.0 \n",
            "\n",
            "keep 0.2222222222222222 \n",
            "\n",
            "many 1.0 \n",
            "\n",
            "belfry 0.5 \n",
            "\n",
            "even 0.42857142857142855 \n",
            "\n",
            "scrape 0.0 \n",
            "\n",
            "say 0.19047619047619047 \n",
            "\n",
            "retired 1.0 \n",
            "\n",
            "worker 1.0 \n",
            "\n",
            "try 0.8 \n",
            "\n",
            "train 0.6666666666666666 \n",
            "\n",
            "disco 0.0 \n",
            "\n",
            "dance 0.0 \n",
            "\n",
            "just 0.2857142857142857 \n",
            "\n",
            "drift 0.5 \n",
            "\n",
            "away 0.5 \n",
            "\n",
            "worry 1.0 \n",
            "\n",
            "old_age 1.0 \n",
            "\n",
            "youth 0.0 \n",
            "\n",
            "diminish 1.0 \n",
            "\n",
            "rank 0.0 \n",
            "\n",
            "history 0.5 \n",
            "\n",
            "side 0.0 \n",
            "\n",
            "nationwide 1.0 \n",
            "\n",
            "survey 1.0 \n",
            "\n",
            "take 0.3125 \n",
            "\n",
            "year 1.0 \n",
            "\n",
            "ago 1.0 \n",
            "\n",
            "nearly 1.0 \n",
            "\n",
            "no 0.0 \n",
            "\n",
            "longer 1.0 \n",
            "\n",
            "ring 0.0 \n",
            "\n",
            "easy 1.0 \n",
            "\n",
            "see 0.5 \n",
            "\n",
            "less 1.0 \n",
            "\n",
            "complicated 1.0 \n",
            "\n",
            "version 1.0 \n",
            "\n",
            "play 0.0 \n",
            "\n",
            "tune 1.0 \n",
            "\n",
            "carillon 0.0 \n",
            "\n",
            "continental 1.0 \n",
            "\n",
            "europe 0.5 \n",
            "\n",
            "consider 0.3333333333333333 \n",
            "\n",
            "childish 1.0 \n",
            "\n",
            "fit 0.0 \n",
            "\n",
            "foreigner 1.0 \n",
            "\n",
            "invent 1.0 \n",
            "\n",
            "require 0.75 \n",
            "\n",
            "physical 1.0 \n",
            "\n",
            "weigh 1.0 \n",
            "\n",
            "more 0.7222222222222222 \n",
            "\n",
            "ton 1.0 \n",
            "\n",
            "combine 1.0 \n",
            "\n",
            "intense 1.0 \n",
            "\n",
            "mental 1.0 \n",
            "\n",
            "concentration 0.5 \n",
            "\n",
            "start 0.6666666666666666 \n",
            "\n",
            "round 0.0 \n",
            "\n",
            "high-pitched 1.0 \n",
            "\n",
            "low 0.25 \n",
            "\n",
            "simple 0.0 \n",
            "\n",
            "descend 1.0 \n",
            "\n",
            "scale 0.0 \n",
            "\n",
            "use 0.975609756097561 \n",
            "\n",
            "large 0.75 \n",
            "\n",
            "then 0.75 \n",
            "\n",
            "signal 0.0 \n",
            "\n",
            "begin 0.5625 \n",
            "\n",
            "vary 0.0 \n",
            "\n",
            "order 0.0 \n",
            "\n",
            "alter 1.0 \n",
            "\n",
            "steady 1.0 \n",
            "\n",
            "rhythm 0.5 \n",
            "\n",
            "striking 0.0 \n",
            "\n",
            "variation 0.0 \n",
            "\n",
            "change 0.42857142857142855 \n",
            "\n",
            "occur 0.75 \n",
            "\n",
            "rule 0.2222222222222222 \n",
            "\n",
            "state 0.5909090909090909 \n",
            "\n",
            "memorize 1.0 \n",
            "\n",
            "pattern 0.16666666666666666 \n",
            "\n",
            "know 0.6896551724137931 \n",
            "\n",
            "method 1.0 \n",
            "\n",
            "name 1.0 \n",
            "\n",
            "series 1.0 \n",
            "\n",
            "so 0.5555555555555556 \n",
            "\n",
            "about 1.0 \n",
            "\n",
            "hour 0.875 \n",
            "\n",
            "look 0.5333333333333333 \n",
            "\n",
            "thursday 1.0 \n",
            "\n",
            "night 1.0 \n",
            "\n",
            "practice 0.8 \n",
            "\n",
            "district 1.0 \n",
            "\n",
            "give 0.0 \n",
            "\n",
            "idea 0.7142857142857143 \n",
            "\n",
            "work 0.2777777777777778 \n",
            "\n",
            "involve 0.0 \n",
            "\n",
            "circle 0.0 \n",
            "\n",
            "foot 0.4 \n",
            "\n",
            "ahead 1.0 \n",
            "\n",
            "stance 1.0 \n",
            "\n",
            "small 0.75 \n",
            "\n",
            "hole 0.0 \n",
            "\n",
            "high 0.5 \n",
            "\n",
            "ceiling 1.0 \n",
            "\n",
            "chamber 0.5 \n",
            "\n",
            "speak 0.0 \n",
            "\n",
            "snake 0.0 \n",
            "\n",
            "seem 0.6666666666666666 \n",
            "\n",
            "make 0.1935483870967742 \n",
            "\n",
            "much 1.0 \n",
            "\n",
            "muffle 0.0 \n",
            "\n",
            "totally 1.0 \n",
            "\n",
            "absorb 0.0 \n",
            "\n",
            "stare 1.0 \n",
            "\n",
            "straight 1.0 \n",
            "\n",
            "watch 0.6666666666666666 \n",
            "\n",
            "thus 0.5 \n",
            "\n",
            "time 0.23333333333333334 \n",
            "\n",
            "far 0.5 \n",
            "\n",
            "above 0.0 \n",
            "\n",
            "huge 1.0 \n",
            "\n",
            "bronze 1.0 \n",
            "\n",
            "mount 1.0 \n",
            "\n",
            "wheel 1.0 \n",
            "\n",
            "swing 0.0 \n",
            "\n",
            "madly 0.0 \n",
            "\n",
            "full 0.5 \n",
            "\n",
            "degree 0.6 \n",
            "\n",
            "end 0.18181818181818182 \n",
            "\n",
            "surprisingly 1.0 \n",
            "\n",
            "inverted 0.0 \n",
            "\n",
            "position 0.0 \n",
            "\n",
            "skilled 1.0 \n",
            "\n",
            "wrist 1.0 \n",
            "\n",
            "advance 0.0 \n",
            "\n",
            "retard 0.0 \n",
            "\n",
            "swap 1.0 \n",
            "\n",
            "place 0.4 \n",
            "\n",
            "well-known 1.0 \n",
            "\n",
            "detective_story 1.0 \n",
            "\n",
            "novelist 1.0 \n",
            "\n",
            "describe 0.3333333333333333 \n",
            "\n",
            "passion 0.5 \n",
            "\n",
            "find 0.13793103448275862 \n",
            "\n",
            "satisfaction 1.0 \n",
            "\n",
            "mathematical 0.75 \n",
            "\n",
            "completeness 1.0 \n",
            "\n",
            "mechanical 1.0 \n",
            "\n",
            "perfection 1.0 \n",
            "\n",
            "add 0.5714285714285714 \n",
            "\n",
            "fill 0.5 \n",
            "\n",
            "solemn 0.0 \n",
            "\n",
            "come 0.29411764705882354 \n",
            "\n",
            "intricate 1.0 \n",
            "\n",
            "ritual 1.0 \n",
            "\n",
            "perform 1.0 \n",
            "\n",
            "become 0.5714285714285714 \n",
            "\n",
            "bit 0.5 \n",
            "\n",
            "obsession 1.0 \n",
            "\n",
            "admit 1.0 \n",
            "\n",
            "master 0.5 \n",
            "\n",
            "best 1.0 \n",
            "\n",
            "female 0.0 \n",
            "\n",
            "usually 1.0 \n",
            "\n",
            "stay 0.0 \n",
            "\n",
            "however 0.6666666666666666 \n",
            "\n",
            "often 1.0 \n",
            "\n",
            "think_of 0.0 \n",
            "\n",
            "stick_on 1.0 \n",
            "\n",
            "bottom 0.6666666666666666 \n",
            "\n",
            "complete 1.0 \n",
            "\n",
            "work_up 0.0 \n",
            "\n",
            "sweat 1.0 \n",
            "\n",
            "skip 0.0 \n",
            "\n",
            "leave 0.0 \n",
            "\n",
            "worship 1.0 \n",
            "\n",
            "below 0.5 \n",
            "\n",
            "well 1.0 \n",
            "\n",
            "cleric 1.0 \n",
            "\n",
            "membership 1.0 \n",
            "\n",
            "church_of_england 1.0 \n",
            "\n",
            "steadily 1.0 \n",
            "\n",
            "dwindle 1.0 \n",
            "\n",
            "vicar 0.0 \n",
            "\n",
            "press 0.0 \n",
            "\n",
            "equally 1.0 \n",
            "\n",
            "attend 1.0 \n",
            "\n",
            "get 0.25 \n",
            "\n",
            "sack 0.0 \n",
            "\n",
            "entire 1.0 \n",
            "\n",
            "promptly 1.0 \n",
            "\n",
            "set_up 1.0 \n",
            "\n",
            "protest 0.0 \n",
            "\n",
            "club 0.3333333333333333 \n",
            "\n",
            "treat 0.0 \n",
            "\n",
            "sort 0.25 \n",
            "\n",
            "separate 0.6666666666666666 \n",
            "\n",
            "premises 0.0 \n",
            "\n",
            "new 0.4375 \n",
            "\n",
            "congregation 1.0 \n",
            "\n",
            "still 1.0 \n",
            "\n",
            "enough 1.0 \n",
            "\n",
            "fall 0.6666666666666666 \n",
            "\n",
            "silent 1.0 \n",
            "\n",
            "follow 0.0 \n",
            "\n",
            "attendance 1.0 \n",
            "\n",
            "refuse 1.0 \n",
            "\n",
            "talk_about 0.0 \n",
            "\n",
            "nearby 1.0 \n",
            "\n",
            "feel 0.3125 \n",
            "\n",
            "fault 0.0 \n",
            "\n",
            "stairs 1.0 \n",
            "\n",
            "locate 0.0 \n",
            "\n",
            "next 1.0 \n",
            "\n",
            "altar 0.0 \n",
            "\n",
            "crunch 1.0 \n",
            "\n",
            "bang 1.0 \n",
            "\n",
            "very 1.0 \n",
            "\n",
            "obvious 1.0 \n",
            "\n",
            "exit 0.0 \n",
            "\n",
            "prayer 1.0 \n",
            "\n",
            "mixed 1.0 \n",
            "\n",
            "feelings 1.0 \n",
            "\n",
            "issue 0.9166666666666666 \n",
            "\n",
            "active 0.2 \n",
            "\n",
            "bell_ringer 0.0 \n",
            "\n",
            "draw 0.0 \n",
            "\n",
            "people 1.0 \n",
            "\n",
            "hope 0.5 \n",
            "\n",
            "life 0.0625 \n",
            "\n",
            "parliament 1.0 \n",
            "\n",
            "aim 1.0 \n",
            "\n",
            "improve 1.0 \n",
            "\n",
            "relations 1.0 \n",
            "\n",
            "president 0.1 \n",
            "\n",
            "student 1.0 \n",
            "\n",
            "theological 1.0 \n",
            "\n",
            "college 0.0 \n",
            "\n",
            "joy 1.0 \n",
            "\n",
            "bell_ringing 0.0 \n",
            "\n",
            "shortly 0.0 \n",
            "\n",
            "publish 0.8 \n",
            "\n",
            "country 0.9354838709677419 \n",
            "\n",
            "entitle 0.0 \n",
            "\n",
            "care 0.5 \n",
            "\n",
            "recognize 0.0 \n",
            "\n",
            "priority 1.0 \n",
            "\n",
            "experience 0.6666666666666666 \n",
            "\n",
            "attack 0.0 \n",
            "\n",
            "greater 1.0 \n",
            "\n",
            "problem 0.7857142857142857 \n",
            "\n",
            "lack 1.0 \n",
            "\n",
            "need 0.375 \n",
            "\n",
            "parish 1.0 \n",
            "\n",
            "inner_city 1.0 \n",
            "\n",
            "council 0.0 \n",
            "\n",
            "program 0.2 \n",
            "\n",
            "attract 1.0 \n",
            "\n",
            "partly 1.0 \n",
            "\n",
            "successful 1.0 \n",
            "\n",
            "right 0.1111111111111111 \n",
            "\n",
            "lucky 1.0 \n",
            "\n",
            "bright 1.0 \n",
            "\n",
            "sign 0.6666666666666666 \n",
            "\n",
            "grow 0.0 \n",
            "\n",
            "number 1.0 \n",
            "\n",
            "enter 0.3333333333333333 \n",
            "\n",
            "third 1.0 \n",
            "\n",
            "accept 0.3333333333333333 \n",
            "\n",
            "everywhere 1.0 \n",
            "\n",
            "found 1.0 \n",
            "\n",
            "fact 1.0 \n",
            "\n",
            "particularly 1.0 \n",
            "\n",
            "galling 1.0 \n",
            "\n",
            "source 0.0 \n",
            "\n",
            "prestigious 0.0 \n",
            "\n",
            "file 1.0 \n",
            "\n",
            "suit 0.0 \n",
            "\n",
            "extent 1.0 \n",
            "\n",
            "surface 0.0 \n",
            "\n",
            "summer 1.0 \n",
            "\n",
            "letter 0.6666666666666666 \n",
            "\n",
            "weekly 1.0 \n",
            "\n",
            "newspaper 1.0 \n",
            "\n",
            "writer 0.0 \n",
            "\n",
            "balanced 1.0 \n",
            "\n",
            "male 0.0 \n",
            "\n",
            "remark 0.0 \n",
            "\n",
            "frequency 1.0 \n",
            "\n",
            "faint 0.3333333333333333 \n",
            "\n",
            "suggest 0.5 \n",
            "\n",
            "settle 0.0 \n",
            "\n",
            "back 0.8 \n",
            "\n",
            "traditional 0.0 \n",
            "\n",
            "role 0.7777777777777778 \n",
            "\n",
            "tea 1.0 \n",
            "\n",
            "meeting 0.5 \n",
            "\n",
            "torrent 1.0 \n",
            "\n",
            "reply 1.0 \n",
            "\n",
            "observe 0.0 \n",
            "\n",
            "average 0.2857142857142857 \n",
            "\n",
            "quite 1.0 \n",
            "\n",
            "lot 1.0 \n",
            "\n",
            "desire 1.0 \n",
            "\n",
            "badly 0.0 \n",
            "\n",
            "dress 0.0 \n",
            "\n",
            "decorate 0.0 \n",
            "\n",
            "frequently 1.0 \n",
            "\n",
            "unwashed 1.0 \n",
            "\n",
            "flatulent 0.0 \n",
            "\n",
            "write 0.0 \n",
            "\n",
            "never 1.0 \n",
            "\n",
            "lady 1.0 \n",
            "\n",
            "die 1.0 \n",
            "\n",
            "bless 0.0 \n",
            "\n",
            "medical 0.5 \n",
            "\n",
            "scientist 1.0 \n",
            "\n",
            "handful 0.0 \n",
            "\n",
            "damage 1.0 \n",
            "\n",
            "unleash 0.0 \n",
            "\n",
            "chaotic 1.0 \n",
            "\n",
            "growth 0.7777777777777778 \n",
            "\n",
            "cell 0.6153846153846154 \n",
            "\n",
            "characterize 0.0 \n",
            "\n",
            "cancer 1.0 \n",
            "\n",
            "discovery 0.6666666666666666 \n",
            "\n",
            "recent 1.0 \n",
            "\n",
            "month 0.46153846153846156 \n",
            "\n",
            "paint_a_picture 1.0 \n",
            "\n",
            "startling 1.0 \n",
            "\n",
            "develop 0.0 \n",
            "\n",
            "emerge 0.25 \n",
            "\n",
            "understanding 1.0 \n",
            "\n",
            "expect 1.0 \n",
            "\n",
            "produce 0.4 \n",
            "\n",
            "array 0.0 \n",
            "\n",
            "strategy 1.0 \n",
            "\n",
            "future 1.0 \n",
            "\n",
            "treatment 1.0 \n",
            "\n",
            "prevention 1.0 \n",
            "\n",
            "already 1.0 \n",
            "\n",
            "test 0.625 \n",
            "\n",
            "base 1.0 \n",
            "\n",
            "newly 1.0 \n",
            "\n",
            "identify 0.0 \n",
            "\n",
            "predict 0.3333333333333333 \n",
            "\n",
            "otherwise 1.0 \n",
            "\n",
            "healthy 1.0 \n",
            "\n",
            "individual 1.0 \n",
            "\n",
            "likely 1.0 \n",
            "\n",
            "researcher 1.0 \n",
            "\n",
            "trigger 1.0 \n",
            "\n",
            "colon 1.0 \n",
            "\n",
            "decade 1.0 \n",
            "\n",
            "nothing 1.0 \n",
            "\n",
            "molecular 1.0 \n",
            "\n",
            "level 0.35714285714285715 \n",
            "\n",
            "accumulation 0.5 \n",
            "\n",
            "initiate 1.0 \n",
            "\n",
            "propel 1.0 \n",
            "\n",
            "deadly 1.0 \n",
            "\n",
            "class 0.0 \n",
            "\n",
            "simply 1.0 \n",
            "\n",
            "function 1.0 \n",
            "\n",
            "normally 1.0 \n",
            "\n",
            "protein 1.0 \n",
            "\n",
            "hold 0.3333333333333333 \n",
            "\n",
            "perhaps 1.0 \n",
            "\n",
            "radiation 1.0 \n",
            "\n",
            "chemical 1.0 \n",
            "\n",
            "chance 0.5 \n",
            "\n",
            "accident 0.8 \n",
            "\n",
            "division 0.0 \n",
            "\n",
            "turn 0.3333333333333333 \n",
            "\n",
            "differ 1.0 \n",
            "\n",
            "family 0.5 \n",
            "\n",
            "discover 0.3333333333333333 \n",
            "\n",
            "early 0.75 \n",
            "\n",
            "present 0.0 \n",
            "\n",
            "normal 1.0 \n",
            "\n",
            "cancerous 1.0 \n",
            "\n",
            "cause 1.0 \n",
            "\n",
            "believe 0.3333333333333333 \n",
            "\n",
            "type 1.0 \n",
            "\n",
            "proliferate 0.0 \n",
            "\n",
            "inherit 0.0 \n",
            "\n",
            "copy 0.0 \n",
            "\n",
            "parent 1.0 \n",
            "\n",
            "control 0.5 \n",
            "\n",
            "arise 1.0 \n",
            "\n",
            "impaired 1.0 \n",
            "\n",
            "person 1.0 \n",
            "\n",
            "bear 1.0 \n",
            "\n",
            "defective 1.0 \n",
            "\n",
            "especially 1.0 \n",
            "\n",
            "lose 1.0 \n",
            "\n",
            "emerging 0.0 \n",
            "\n",
            "genetic 0.1111111111111111 \n",
            "\n",
            "able 1.0 \n",
            "\n",
            "spot 1.0 \n",
            "\n",
            "such 1.0 \n",
            "\n",
            "usher_in 1.0 \n",
            "\n",
            "age 0.75 \n",
            "\n",
            "predictive 1.0 \n",
            "\n",
            "diagnosis 1.0 \n",
            "\n",
            "beneficiary 1.0 \n",
            "\n",
            "finding 0.0 \n",
            "\n",
            "couple 0.25 \n",
            "\n",
            "before 1.0 \n",
            "\n",
            "pregnant 1.0 \n",
            "\n",
            "child 0.42857142857142855 \n",
            "\n",
            "risk 1.0 \n",
            "\n",
            "eye 1.0 \n",
            "\n",
            "birth 1.0 \n",
            "\n",
            "carry 0.0 \n",
            "\n",
            "damaged 1.0 \n",
            "\n",
            "rare 1.0 \n",
            "\n",
            "tumor 1.0 \n",
            "\n",
            "mother 1.0 \n",
            "\n",
            "suffer 0.3333333333333333 \n",
            "\n",
            "fate 1.0 \n",
            "\n",
            "baby 1.0 \n",
            "\n",
            "isolation 0.0 \n",
            "\n",
            "possible 0.8 \n",
            "\n",
            "last 1.0 \n",
            "\n",
            "january 1.0 \n",
            "\n",
            "find_out 0.0 \n",
            "\n",
            "threat 1.0 \n",
            "\n",
            "face 0.6666666666666666 \n",
            "\n",
            "show 0.08333333333333333 \n",
            "\n",
            "little 0.8 \n",
            "\n",
            "therefore 1.0 \n",
            "\n",
            "reliably 1.0 \n",
            "\n",
            "important 0.42857142857142855 \n",
            "\n",
            "initial 0.5 \n",
            "\n",
            "retinal 1.0 \n",
            "\n",
            "doctor 1.0 \n",
            "\n",
            "open 0.5 \n",
            "\n",
            "study 0.7857142857142857 \n",
            "\n",
            "explode 0.0 \n",
            "\n",
            "turn_out 0.0 \n",
            "\n",
            "tragic 1.0 \n",
            "\n",
            "uncommon 1.0 \n",
            "\n",
            "fundamental 1.0 \n",
            "\n",
            "insight 1.0 \n",
            "\n",
            "basic 0.6666666666666666 \n",
            "\n",
            "director 1.0 \n",
            "\n",
            "public 0.6666666666666666 \n",
            "\n",
            "concerned 1.0 \n",
            "\n",
            "research 1.0 \n",
            "\n",
            "result 0.4166666666666667 \n",
            "\n",
            "there 1.0 \n",
            "\n",
            "soon 1.0 \n",
            "\n",
            "spring 1.0 \n",
            "\n",
            "report 0.5909090909090909 \n",
            "\n",
            "impair 1.0 \n",
            "\n",
            "team 1.0 \n",
            "\n",
            "evidence 0.8888888888888888 \n",
            "\n",
            "tissue 1.0 \n",
            "\n",
            "lung 1.0 \n",
            "\n",
            "breast 0.0 \n",
            "\n",
            "common 0.0 \n",
            "\n",
            "lethal 1.0 \n",
            "\n",
            "form 0.0 \n",
            "\n",
            "disease 1.0 \n",
            "\n",
            "collectively 1.0 \n",
            "\n",
            "kill 0.6666666666666666 \n",
            "\n",
            "almost 1.0 \n",
            "\n",
            "american 1.0 \n",
            "\n",
            "dozen 1.0 \n",
            "\n",
            "laboratory 1.0 \n",
            "\n",
            "u.s. 0.14285714285714285 \n",
            "\n",
            "canada 1.0 \n",
            "\n",
            "race 0.8571428571428571 \n",
            "\n",
            "alone 0.75 \n",
            "\n",
            "combination 1.0 \n",
            "\n",
            "appear 0.5 \n",
            "\n",
            "crucial 0.0 \n",
            "\n",
            "development 0.25 \n",
            "\n",
            "scourge 1.0 \n",
            "\n",
            "brain 0.6666666666666666 \n",
            "\n",
            "skin 1.0 \n",
            "\n",
            "kidney 1.0 \n",
            "\n",
            "prostate 1.0 \n",
            "\n",
            "cervix 0.0 \n",
            "\n",
            "explain 0.8 \n",
            "\n",
            "finally 1.0 \n",
            "\n",
            "haunt 1.0 \n",
            "\n",
            "certain 0.8 \n",
            "\n",
            "story 0.0 \n",
            "\n",
            "go_back 0.0 \n",
            "\n",
            "propose 1.0 \n",
            "\n",
            "stem 1.0 \n",
            "\n",
            "defect 0.0 \n",
            "\n",
            "theorize 0.0 \n",
            "\n",
            "infant 1.0 \n",
            "\n",
            "second 0.9166666666666666 \n",
            "\n",
            "way 0.5 \n",
            "\n",
            "prove 0.0 \n",
            "\n",
            "theory 0.6666666666666666 \n",
            "\n",
            "ferret_out 1.0 \n",
            "\n",
            "specific 0.5 \n",
            "\n",
            "microscope 1.0 \n",
            "\n",
            "pair 0.0 \n",
            "\n",
            "contain 1.0 \n",
            "\n",
            "occasionally 1.0 \n",
            "\n",
            "gross 0.0 \n",
            "\n",
            "visible 1.0 \n",
            "\n",
            "no. 1.0 \n",
            "\n",
            "actually 0.3333333333333333 \n",
            "\n",
            "miss 0.0 \n",
            "\n",
            "assume 1.0 \n",
            "\n",
            "piece 1.0 \n",
            "\n",
            "loss 0.2857142857142857 \n",
            "\n",
            "critical 0.0 \n",
            "\n",
            "set_off 1.0 \n",
            "\n",
            "scientific 0.5 \n",
            "\n",
            "lead 0.375 \n",
            "\n",
            "geneticist 1.0 \n",
            "\n",
            "answer 0.3333333333333333 \n",
            "\n",
            "battery 0.0 \n",
            "\n",
            "material 1.0 \n",
            "\n",
            "track 0.5 \n",
            "\n",
            "presence 0.5 \n",
            "\n",
            "analyze 1.0 \n",
            "\n",
            "extract 1.0 \n",
            "\n",
            "exact 1.0 \n",
            "\n",
            "area 0.5 \n",
            "\n",
            "medicine 0.0 \n",
            "\n",
            "eruption 1.0 \n",
            "\n",
            "satisfying 0.0 \n",
            "\n",
            "true 1.0 \n",
            "\n",
            "audacious 1.0 \n",
            "\n",
            "claim 0.08333333333333333 \n",
            "\n",
            "young 1.0 \n",
            "\n",
            "biologist 1.0 \n",
            "\n",
            "set_out 1.0 \n",
            "\n",
            "repeat 0.0 \n",
            "\n",
            "experiment 1.0 \n",
            "\n",
            "dual 1.0 \n",
            "\n",
            "childhood 1.0 \n",
            "\n",
            "turn_to 1.0 \n",
            "\n",
            "attention 0.5 \n",
            "\n",
            "big 1.0 \n",
            "\n",
            "killer 0.5 \n",
            "\n",
            "multiple 1.0 \n",
            "\n",
            "stage 0.6666666666666666 \n",
            "\n",
            "precede 0.0 \n",
            "\n",
            "polyp 0.0 \n",
            "\n",
            "case 0.23809523809523808 \n",
            "\n",
            "increasingly 1.0 \n",
            "\n",
            "identifiable 1.0 \n",
            "\n",
            "progress 0.25 \n",
            "\n",
            "severe 0.75 \n",
            "\n",
            "tedious 1.0 \n",
            "\n",
            "frustrating 1.0 \n",
            "\n",
            "probe 1.0 \n",
            "\n",
            "search 0.75 \n",
            "\n",
            "confusing 0.0 \n",
            "\n",
            "variety 0.5 \n",
            "\n",
            "exist 1.0 \n",
            "\n",
            "gradually 1.0 \n",
            "\n",
            "coherent 1.0 \n",
            "\n",
            "picture 0.0 \n",
            "\n",
            "knock_out 0.5 \n",
            "\n",
            "delete 1.0 \n",
            "\n",
            "malignancy 1.0 \n",
            "\n",
            "clear 0.3333333333333333 \n",
            "\n",
            "galvanize 0.0 \n",
            "\n",
            "confirm 1.0 \n",
            "\n",
            "yet 1.0 \n",
            "\n",
            "nail 0.0 \n",
            "\n",
            "identity 0.0 \n",
            "\n",
            "flip 0.0 \n",
            "\n",
            "full-blown 1.0 \n",
            "\n",
            "focus_on 1.0 \n",
            "\n",
            "experimentally 1.0 \n",
            "\n",
            "down 0.5 \n",
            "\n",
            "length 0.0 \n",
            "\n",
            "probably 1.0 \n",
            "\n",
            "constitute 1.0 \n",
            "\n",
            "winter 1.0 \n",
            "\n",
            "dubious 1.0 \n",
            "\n",
            "over 1.0 \n",
            "\n",
            "doubt 1.0 \n",
            "\n",
            "several 1.0 \n",
            "\n",
            "earlier 1.0 \n",
            "\n",
            "mouse 1.0 \n",
            "\n",
            "transform 1.0 \n",
            "\n",
            "one 0.0 \n",
            "\n",
            "exactly 1.0 \n",
            "\n",
            "promote 1.0 \n",
            "\n",
            "suppress 0.0 \n",
            "\n",
            "compare 0.875 \n",
            "\n",
            "human 0.5 \n",
            "\n",
            "identical 1.0 \n",
            "\n",
            "suddenly 0.0 \n",
            "\n",
            "put 0.6666666666666666 \n",
            "\n",
            "obscure 1.0 \n",
            "\n",
            "formation 0.0 \n",
            "\n",
            "leader 1.0 \n",
            "\n",
            "too 0.3333333333333333 \n",
            "\n",
            "strike 0.0 \n",
            "\n",
            "implicate 1.0 \n",
            "\n",
            "week 0.2 \n",
            "\n",
            "colleague 0.5 \n",
            "\n",
            "lab 1.0 \n",
            "\n",
            "as 1.0 \n",
            "\n",
            "unpublished 1.0 \n",
            "\n",
            "at_the_same_time 0.0 \n",
            "\n",
            "rush 0.0 \n",
            "\n",
            "close 1.0 \n",
            "\n",
            "think 0.2727272727272727 \n",
            "\n",
            "somewhere 1.0 \n",
            "\n",
            "recently 1.0 \n",
            "\n",
            "speculate 0.0 \n",
            "\n",
            "major 0.8 \n",
            "\n",
            "pharmaceutical 0.0 \n",
            "\n",
            "company 0.8421052631578947 \n",
            "\n",
            "unit 0.0 \n",
            "\n",
            "collaborate 1.0 \n",
            "\n",
            "hunter 0.0 \n",
            "\n",
            "anticipated 1.0 \n",
            "\n",
            "maybe 1.0 \n",
            "\n",
            "therapy 1.0 \n",
            "\n",
            "drug 1.0 \n",
            "\n",
            "slow 0.0 \n",
            "\n",
            "reverse 0.5 \n",
            "\n",
            "administer 0.0 \n",
            "\n",
            "patient 1.0 \n",
            "\n",
            "replace 1.0 \n",
            "\n",
            "come_close 1.0 \n",
            "\n",
            "in_any_case 1.0 \n",
            "\n",
            "witness 1.0 \n",
            "\n",
            "step 1.0 \n",
            "\n",
            "genesis 1.0 \n",
            "\n",
            "teach 1.0 \n",
            "\n",
            "read 0.8333333333333334 \n",
            "\n",
            "reckon 0.0 \n",
            "\n",
            "want 1.0 \n",
            "\n",
            "reason 0.125 \n",
            "\n",
            "effective 0.5 \n",
            "\n",
            "education 0.7333333333333333 \n",
            "\n",
            "relinquish 0.0 \n",
            "\n",
            "cherished 1.0 \n",
            "\n",
            "metaphysical 0.0 \n",
            "\n",
            "belief 1.0 \n",
            "\n",
            "nature 1.0 \n",
            "\n",
            "in_particular 1.0 \n",
            "\n",
            "violate 1.0 \n",
            "\n",
            "interest 0.6 \n",
            "\n",
            "dominate 0.0 \n",
            "\n",
            "educational 1.0 \n",
            "\n",
            "establishment 0.0 \n",
            "\n",
            "politician 1.0 \n",
            "\n",
            "blasphemous 0.0 \n",
            "\n",
            "challenge 0.5714285714285714 \n",
            "\n",
            "example 0.5 \n",
            "\n",
            "ask 0.3333333333333333 \n",
            "\n",
            "sample 0.0 \n",
            "\n",
            "wish 0.6 \n",
            "\n",
            "elementary_school 1.0 \n",
            "\n",
            "encourage 0.5 \n",
            "\n",
            "creativity 1.0 \n",
            "\n",
            "of_course 1.0 \n",
            "\n",
            "mean 0.6666666666666666 \n",
            "\n",
            "specifically 1.0 \n",
            "\n",
            "in_practice 1.0 \n",
            "\n",
            "end_up 1.0 \n",
            "\n",
            "equate 1.0 \n",
            "\n",
            "self-esteem 1.0 \n",
            "\n",
            "generation 1.0 \n",
            "\n",
            "ignorance 1.0 \n",
            "\n",
            "intellectual 0.0 \n",
            "\n",
            "incompetence 0.0 \n",
            "\n",
            "match 0.125 \n",
            "\n",
            "good 0.0 \n",
            "\n",
            "opinion 1.0 \n",
            "\n",
            "whole 1.0 \n",
            "\n",
            "notion 0.75 \n",
            "\n",
            "romantic 0.0 \n",
            "\n",
            "rebellion 0.0 \n",
            "\n",
            "disciplined 1.0 \n",
            "\n",
            "instruction 0.5 \n",
            "\n",
            "regard_as 1.0 \n",
            "\n",
            "authoritarian 1.0 \n",
            "\n",
            "repression 0.0 \n",
            "\n",
            "frustration 0.0 \n",
            "\n",
            "latent 1.0 \n",
            "\n",
            "talent 0.5 \n",
            "\n",
            "wonderful 1.0 \n",
            "\n",
            "potentiality 0.0 \n",
            "\n",
            "inherent 1.0 \n",
            "\n",
            "soul 1.0 \n",
            "\n",
            "surprising 1.0 \n",
            "\n",
            "attractive 1.0 \n",
            "\n",
            "fortunately 1.0 \n",
            "\n",
            "decent 1.0 \n",
            "\n",
            "traditionally 1.0 \n",
            "\n",
            "understand 1.0 \n",
            "\n",
            "common_sense 1.0 \n",
            "\n",
            "demand 0.0 \n",
            "\n",
            "commitment 0.0 \n",
            "\n",
            "survive 0.6666666666666666 \n",
            "\n",
            "adolescent 0.0 \n",
            "\n",
            "illiteracy 1.0 \n",
            "\n",
            "determine 0.0 \n",
            "\n",
            "allow 0.5 \n",
            "\n",
            "prevail 0.0 \n",
            "\n",
            "illusion 0.0 \n",
            "\n",
            "share 0.5 \n",
            "\n",
            "fight 0.5 \n",
            "\n",
            "complex 1.0 \n",
            "\n",
            "progressive 0.0 \n",
            "\n",
            "interesting 1.0 \n",
            "\n",
            "agreeable 0.0 \n",
            "\n",
            "teacher 1.0 \n",
            "\n",
            "nice 1.0 \n",
            "\n",
            "personality 1.0 \n",
            "\n",
            "minimize 0.0 \n",
            "\n",
            "irksome 1.0 \n",
            "\n",
            "disappointing 1.0 \n",
            "\n",
            "provide 0.8 \n",
            "\n",
            "profession 1.0 \n",
            "\n",
            "pass 0.2857142857142857 \n",
            "\n",
            "course 1.0 \n",
            "\n",
            "psychology 1.0 \n",
            "\n",
            "philosophy 0.5 \n",
            "\n",
            "all 1.0 \n",
            "\n",
            "fairly 0.0 \n",
            "\n",
            "pap 0.0 \n",
            "\n",
            "unfair 1.0 \n",
            "\n",
            "dump 0.0 \n",
            "\n",
            "distinct 1.0 \n",
            "\n",
            "on_the_whole 1.0 \n",
            "\n",
            "seriously 1.0 \n",
            "\n",
            "commit 1.0 \n",
            "\n",
            "conscientious 1.0 \n",
            "\n",
            "teaching 0.0 \n",
            "\n",
            "few 1.0 \n",
            "\n",
            "job 0.75 \n",
            "\n",
            "remember 1.0 \n",
            "\n",
            "truly 1.0 \n",
            "\n",
            "inspiring 0.0 \n",
            "\n",
            "proceed 0.0 \n",
            "\n",
            "merely 1.0 \n",
            "\n",
            "competent 1.0 \n",
            "\n",
            "sense 0.3333333333333333 \n",
            "\n",
            "brilliant 1.0 \n",
            "\n",
            "in_the_first_place 1.0 \n",
            "\n",
            "serve 0.5 \n",
            "\n",
            "factor 1.0 \n",
            "\n",
            "crisis 1.0 \n",
            "\n",
            "equity 0.0 \n",
            "\n",
            "silly 1.0 \n",
            "\n",
            "libel 1.0 \n",
            "\n",
            "educate 1.0 \n",
            "\n",
            "better 0.5 \n",
            "\n",
            "kind 1.0 \n",
            "\n",
            "union 1.0 \n",
            "\n",
            "mind 1.0 \n",
            "\n",
            "spread 0.25 \n",
            "\n",
            "narrow 0.0 \n",
            "\n",
            "purpose 1.0 \n",
            "\n",
            "useful 1.0 \n",
            "\n",
            "help 1.0 \n",
            "\n",
            "friendly 0.5 \n",
            "\n",
            "posture 0.0 \n",
            "\n",
            "behalf 1.0 \n",
            "\n",
            "shred 1.0 \n",
            "\n",
            "thing 0.23076923076923078 \n",
            "\n",
            "equal 1.0 \n",
            "\n",
            "salary 1.0 \n",
            "\n",
            "differential 0.0 \n",
            "\n",
            "sure 1.0 \n",
            "\n",
            "serious 1.0 \n",
            "\n",
            "school 0.8888888888888888 \n",
            "\n",
            "scattered 1.0 \n",
            "\n",
            "nation 1.0 \n",
            "\n",
            "poor 0.0 \n",
            "\n",
            "conversely 1.0 \n",
            "\n",
            "majority 0.5 \n",
            "\n",
            "unsuccessful 1.0 \n",
            "\n",
            "reform 1.0 \n",
            "\n",
            "doom 1.0 \n",
            "\n",
            "beforehand 1.0 \n",
            "\n",
            "really 1.0 \n",
            "\n",
            "assimilate 1.0 \n",
            "\n",
            "knowledge 1.0 \n",
            "\n",
            "respect 1.0 \n",
            "\n",
            "helpful 1.0 \n",
            "\n",
            "political 0.25 \n",
            "\n",
            "mute 1.0 \n",
            "\n",
            "rather 1.0 \n",
            "\n",
            "eloquently 1.0 \n",
            "\n",
            "concern 0.6666666666666666 \n",
            "\n",
            "inevitably 0.0 \n",
            "\n",
            "inclined 1.0 \n",
            "\n",
            "echo 1.0 \n",
            "\n",
            "conventional 1.0 \n",
            "\n",
            "least 1.0 \n",
            "\n",
            "controversial 1.0 \n",
            "\n",
            "governor 1.0 \n",
            "\n",
            "conference 0.75 \n",
            "\n",
            "announce 0.0 \n",
            "\n",
            "comprehensive 0.5 \n",
            "\n",
            "policy 0.6428571428571429 \n",
            "\n",
            "pressing 1.0 \n",
            "\n",
            "advocate 1.0 \n",
            "\n",
            "in_effect 1.0 \n",
            "\n",
            "extend 1.0 \n",
            "\n",
            "system 0.0 \n",
            "\n",
            "desirable 1.0 \n",
            "\n",
            "exposure 0.0 \n",
            "\n",
            "schooling 0.0 \n",
            "\n",
            "performance 0.0 \n",
            "\n",
            "grade 1.0 \n",
            "\n",
            "afterward 1.0 \n",
            "\n",
            "difference 0.6666666666666666 \n",
            "\n",
            "quickly 0.0 \n",
            "\n",
            "wash 0.0 \n",
            "\n",
            "sum_up 1.0 \n",
            "\n",
            "parental 1.0 \n",
            "\n",
            "involvement 1.0 \n",
            "\n",
            "bad 1.0 \n",
            "\n",
            "blame 0.0 \n",
            "\n",
            "limitation 1.0 \n",
            "\n",
            "involved 1.0 \n",
            "\n",
            "home 0.625 \n",
            "\n",
            "kid 0.0 \n",
            "\n",
            "truant 1.0 \n",
            "\n",
            "spend 0.75 \n",
            "\n",
            "scrutinize 1.0 \n",
            "\n",
            "option 0.5 \n",
            "\n",
            "switch 0.0 \n",
            "\n",
            "community 1.0 \n",
            "\n",
            "worse 1.0 \n",
            "\n",
            "locally 0.5 \n",
            "\n",
            "elected 1.0 \n",
            "\n",
            "school_board 1.0 \n",
            "\n",
            "larger 1.0 \n",
            "\n",
            "city 1.0 \n",
            "\n",
            "ambitious 1.0 \n",
            "\n",
            "generally 1.0 \n",
            "\n",
            "corrupt 1.0 \n",
            "\n",
            "invariably 1.0 \n",
            "\n",
            "process 1.0 \n",
            "\n",
            "governance 0.0 \n",
            "\n",
            "institute 0.5 \n",
            "\n",
            "increased 1.0 \n",
            "\n",
            "expenditure 0.3333333333333333 \n",
            "\n",
            "current 0.5 \n",
            "\n",
            "circumstance 0.5 \n",
            "\n",
            "take_place 1.0 \n",
            "\n",
            "classroom 1.0 \n",
            "\n",
            "influence 0.3333333333333333 \n",
            "\n",
            "money 0.6666666666666666 \n",
            "\n",
            "minimal 1.0 \n",
            "\n",
            "tell 0.625 \n",
            "\n",
            "unequivocally 1.0 \n",
            "\n",
            "smaller 1.0 \n",
            "\n",
            "effect 0.7142857142857143 \n",
            "\n",
            "academic 1.0 \n",
            "\n",
            "pupil 1.0 \n",
            "\n",
            "sometimes 1.0 \n",
            "\n",
            "flow 0.3333333333333333 \n",
            "\n",
            "administrative 1.0 \n",
            "\n",
            "structure 0.75 \n",
            "\n",
            "pile 0.0 \n",
            "\n",
            "paperwork 1.0 \n",
            "\n",
            "mystery 1.0 \n",
            "\n",
            "paradox 1.0 \n",
            "\n",
            "real 0.0 \n",
            "\n",
            "increase 0.375 \n",
            "\n",
            "sharply 0.0 \n",
            "\n",
            "past 0.9 \n",
            "\n",
            "decline 0.5 \n",
            "\n",
            "potential 0.6 \n",
            "\n",
            "permit 0.5 \n",
            "\n",
            "graduate 0.0 \n",
            "\n",
            "main 1.0 \n",
            "\n",
            "gifted 1.0 \n",
            "\n",
            "flourish 1.0 \n",
            "\n",
            "clearly 1.0 \n",
            "\n",
            "necessary 1.0 \n",
            "\n",
            "somehow 1.0 \n",
            "\n",
            "absurd 1.0 \n",
            "\n",
            "realize 0.7777777777777778 \n",
            "\n",
            "textbook 1.0 \n",
            "\n",
            "latter 1.0 \n",
            "\n",
            "downright 1.0 \n",
            "\n",
            "insubstantial 1.0 \n",
            "\n",
            "psychological 0.0 \n",
            "\n",
            "sociological 1.0 \n",
            "\n",
            "poorly 1.0 \n",
            "\n",
            "math 1.0 \n",
            "\n",
            "british 1.0 \n",
            "\n",
            "french 1.0 \n",
            "\n",
            "german 1.0 \n",
            "\n",
            "japanese 1.0 \n",
            "\n",
            "appreciate 0.5 \n",
            "\n",
            "bizarre 1.0 \n",
            "\n",
            "situation 1.0 \n",
            "\n",
            "subtract 1.0 \n",
            "\n",
            "learn 1.0 \n",
            "\n",
            "conceptual 1.0 \n",
            "\n",
            "basis 0.25 \n",
            "\n",
            "article 1.0 \n",
            "\n",
            "principal 0.0 \n",
            "\n",
            "measure 0.5 \n",
            "\n",
            "authority 1.0 \n",
            "\n",
            "faculty 0.0 \n",
            "\n",
            "curriculum 1.0 \n",
            "\n",
            "matter 0.8571428571428571 \n",
            "\n",
            "outside 0.5 \n",
            "\n",
            "interference 0.0 \n",
            "\n",
            "govern 0.0 \n",
            "\n",
            "powerful 1.0 \n",
            "\n",
            "head 0.8 \n",
            "\n",
            "go_with 1.0 \n",
            "\n",
            "unambiguous 0.0 \n",
            "\n",
            "structured 1.0 \n",
            "\n",
            "higher 1.0 \n",
            "\n",
            "morale 0.0 \n",
            "\n",
            "superior 1.0 \n",
            "\n",
            "ruffle 0.0 \n",
            "\n",
            "peer 1.0 \n",
            "\n",
            "companion 0.5 \n",
            "\n",
            "bleary 1.0 \n",
            "\n",
            "have 0.43333333333333335 \n",
            "\n",
            "ready 0.0 \n",
            "\n",
            "surprise 1.0 \n",
            "\n",
            "coughing 0.0 \n",
            "\n",
            "drunk 1.0 \n",
            "\n",
            "crazy 1.0 \n",
            "\n",
            "buddy 1.0 \n",
            "\n",
            "drink 0.8 \n",
            "\n",
            "count 1.0 \n",
            "\n",
            "guy 1.0 \n",
            "\n",
            "rocker 0.0 \n",
            "\n",
            "look_away 1.0 \n",
            "\n",
            "strange 1.0 \n",
            "\n",
            "intensity 0.0 \n",
            "\n",
            "cold 0.0 \n",
            "\n",
            "fire 0.75 \n",
            "\n",
            "burn 0.3333333333333333 \n",
            "\n",
            "notice 0.0 \n",
            "\n",
            "wonder 0.4 \n",
            "\n",
            "tie 0.0 \n",
            "\n",
            "stranger 1.0 \n",
            "\n",
            "blur 0.0 \n",
            "\n",
            "drinking 0.0 \n",
            "\n",
            "occasion 0.6666666666666666 \n",
            "\n",
            "come_back 1.0 \n",
            "\n",
            "sink 0.0 \n",
            "\n",
            "thought 0.3333333333333333 \n",
            "\n",
            "cross 1.0 \n",
            "\n",
            "street 0.3333333333333333 \n",
            "\n",
            "light 0.0 \n",
            "\n",
            "roar 0.0 \n",
            "\n",
            "intersection 0.0 \n",
            "\n",
            "brake 1.0 \n",
            "\n",
            "howl 0.0 \n",
            "\n",
            "horn 0.0 \n",
            "\n",
            "blare 1.0 \n",
            "\n",
            "hit 0.0 \n",
            "\n",
            "call_out 0.0 \n",
            "\n",
            "shout 0.5 \n",
            "\n",
            "involuntary 0.0 \n",
            "\n",
            "do 0.07142857142857142 \n",
            "\n",
            "thinking 1.0 \n",
            "\n",
            "as_a_matter_of_fact 1.0 \n",
            "\n",
            "regret 1.0 \n",
            "\n",
            "mouth 1.0 \n",
            "\n",
            "truck 1.0 \n",
            "\n",
            "come_to 0.3333333333333333 \n",
            "\n",
            "angry 1.0 \n",
            "\n",
            "driver 1.0 \n",
            "\n",
            "jump 1.0 \n",
            "\n",
            "cab 1.0 \n",
            "\n",
            "walk 1.0 \n",
            "\n",
            "thank 1.0 \n",
            "\n",
            "arm 1.0 \n",
            "\n",
            "shoulder 1.0 \n",
            "\n",
            "old 0.5714285714285714 \n",
            "\n",
            "friend 1.0 \n",
            "\n",
            "curse 1.0 \n",
            "\n",
            "plot 1.0 \n",
            "\n",
            "ruin 0.5 \n",
            "\n",
            "safe 0.0 \n",
            "\n",
            "record 0.0 \n",
            "\n",
            "save 0.0 \n",
            "\n",
            "word 0.3333333333333333 \n",
            "\n",
            "swear 1.0 \n",
            "\n",
            "heel 0.0 \n",
            "\n",
            "recall 1.0 \n",
            "\n",
            "bluff 0.0 \n",
            "\n",
            "go 0.6363636363636364 \n",
            "\n",
            "go_on 0.0 \n",
            "\n",
            "on_the_way 1.0 \n",
            "\n",
            "favorite 1.0 \n",
            "\n",
            "bar 1.0 \n",
            "\n",
            "insist 0.0 \n",
            "\n",
            "offer 0.6666666666666666 \n",
            "\n",
            "buy 1.0 \n",
            "\n",
            "keep_to_oneself 1.0 \n",
            "\n",
            "sit 1.0 \n",
            "\n",
            "corner 0.2 \n",
            "\n",
            "nurse 0.0 \n",
            "\n",
            "grudge 1.0 \n",
            "\n",
            "decide 1.0 \n",
            "\n",
            "free 0.0 \n",
            "\n",
            "screwball 0.0 \n",
            "\n",
            "uneasy 0.0 \n",
            "\n",
            "first_name 1.0 \n",
            "\n",
            "wait 0.6666666666666666 \n",
            "\n",
            "wink 0.0 \n",
            "\n",
            "continue 0.5 \n",
            "\n",
            "manage 1.0 \n",
            "\n",
            "weak 0.0 \n",
            "\n",
            "laugh 1.0 \n",
            "\n",
            "thanks 1.0 \n",
            "\n",
            "crease 0.0 \n",
            "\n",
            "eyebrow 1.0 \n",
            "\n",
            "return 0.4 \n",
            "\n",
            "favor 0.8 \n",
            "\n",
            "like 0.0 \n",
            "\n",
            "cost 0.5 \n",
            "\n",
            "cent 1.0 \n",
            "\n",
            "go_ahead 1.0 \n",
            "\n",
            "rub 1.0 \n",
            "\n",
            "forehead 1.0 \n",
            "\n",
            "woolly 1.0 \n",
            "\n",
            "humor 1.0 \n",
            "\n",
            "concede 0.0 \n",
            "\n",
            "get_along_with 1.0 \n",
            "\n",
            "landlord 1.0 \n",
            "\n",
            "ride 0.0 \n",
            "\n",
            "listen 1.0 \n",
            "\n",
            "radio 0.0 \n",
            "\n",
            "sing 0.0 \n",
            "\n",
            "bath 0.0 \n",
            "\n",
            "neighbor 1.0 \n",
            "\n",
            "complain 1.0 \n",
            "\n",
            "bother 0.6666666666666666 \n",
            "\n",
            "nod 0.6666666666666666 \n",
            "\n",
            "content 0.0 \n",
            "\n",
            "fine 0.7142857142857143 \n",
            "\n",
            "address 0.0 \n",
            "\n",
            "habit 0.0 \n",
            "\n",
            "movement 0.0 \n",
            "\n",
            "look_like 1.0 \n",
            "\n",
            "suicide 1.0 \n",
            "\n",
            "reasonable 1.0 \n",
            "\n",
            "trouble 0.6 \n",
            "\n",
            "customer 1.0 \n",
            "\n",
            "fly 0.0 \n",
            "\n",
            "smile 0.8333333333333334 \n",
            "\n",
            "line_of_work 1.0 \n",
            "\n",
            "minute 1.0 \n",
            "\n",
            "break 0.0 \n",
            "\n",
            "stand_up 1.0 \n",
            "\n",
            "farewell 1.0 \n",
            "\n",
            "wiry 1.0 \n",
            "\n",
            "slip 0.0 \n",
            "\n",
            "relieve 0.0 \n",
            "\n",
            "departure 1.0 \n",
            "\n",
            "glass 0.4 \n",
            "\n",
            "table 0.42857142857142855 \n",
            "\n",
            "midnight 1.0 \n",
            "\n",
            "apartment 1.0 \n",
            "\n",
            "incident 1.0 \n",
            "\n",
            "fall_asleep 1.0 \n",
            "\n",
            "day 0.4 \n",
            "\n",
            "hangover 1.0 \n",
            "\n",
            "remind 1.0 \n",
            "\n",
            "conversation 1.0 \n",
            "\n",
            "groan 1.0 \n",
            "\n",
            "aspirin 1.0 \n",
            "\n",
            "alive 1.0 \n",
            "\n",
            "as_usual 1.0 \n",
            "\n",
            "tenant 1.0 \n",
            "\n",
            "burden 1.0 \n",
            "\n",
            "shrug 1.0 \n",
            "\n",
            "ignore 1.0 \n",
            "\n",
            "mail 1.0 \n",
            "\n",
            "monthly 1.0 \n",
            "\n",
            "check 0.5 \n",
            "\n",
            "maintenance 0.0 \n",
            "\n",
            "wife 1.0 \n",
            "\n",
            "startle 0.0 \n",
            "\n",
            "front 0.0 \n",
            "\n",
            "lobby 1.0 \n",
            "\n",
            "talk 1.0 \n",
            "\n",
            "spine 1.0 \n",
            "\n",
            "crawl 0.0 \n",
            "\n",
            "foreboding 0.0 \n",
            "\n",
            "premonition 1.0 \n",
            "\n",
            "fellow 0.0 \n",
            "\n",
            "happen 1.0 \n",
            "\n",
            "afternoon 1.0 \n",
            "\n",
            "roof 1.0 \n",
            "\n",
            "errand 1.0 \n",
            "\n",
            "inspection 1.0 \n",
            "\n",
            "nobody 1.0 \n",
            "\n",
            "sickening 0.0 \n",
            "\n",
            "impact 0.14285714285714285 \n",
            "\n",
            "body 0.8333333333333334 \n",
            "\n",
            "smash 1.0 \n",
            "\n",
            "pavement 0.0 \n",
            "\n",
            "basement 1.0 \n",
            "\n",
            "delivery 1.0 \n",
            "\n",
            "entrance 1.0 \n",
            "\n",
            "sudden 1.0 \n",
            "\n",
            "pallor 1.0 \n",
            "\n",
            "blood 1.0 \n",
            "\n",
            "drain 0.0 \n",
            "\n",
            "cheek 1.0 \n",
            "\n",
            "mutter 1.0 \n",
            "\n",
            "terrible 1.0 \n",
            "\n",
            "deliberate 1.0 \n",
            "\n",
            "slowness 0.0 \n",
            "\n",
            "elevator 1.0 \n",
            "\n",
            "pour 0.0 \n",
            "\n",
            "tremble 1.0 \n",
            "\n",
            "hand 0.8 \n",
            "\n",
            "flop 0.5 \n",
            "\n",
            "chair 1.0 \n",
            "\n",
            "while 1.0 \n",
            "\n",
            "question 0.125 \n",
            "\n",
            "convince 1.0 \n",
            "\n",
            "coincidence 1.0 \n",
            "\n",
            "go_to_bed 1.0 \n",
            "\n",
            "happy 1.0 \n",
            "\n",
            "lousy 1.0 \n",
            "\n",
            "sad 1.0 \n",
            "\n",
            "hurt 0.0 \n",
            "\n",
            "feeling 1.0 \n",
            "\n",
            "recurrent 1.0 \n",
            "\n",
            "annoying 1.0 \n",
            "\n",
            "dream 1.0 \n",
            "\n",
            "overtone 0.0 \n",
            "\n",
            "nightmare 1.0 \n",
            "\n",
            "tight 0.0 \n",
            "\n",
            "excuse 1.0 \n",
            "\n",
            "usual 1.0 \n",
            "\n",
            "as_far_as_possible 1.0 \n",
            "\n",
            "talkative 1.0 \n",
            "\n",
            "lush 1.0 \n",
            "\n",
            "wash_down 1.0 \n",
            "\n",
            "pet 0.0 \n",
            "\n",
            "grievance 0.0 \n",
            "\n",
            "bourbon 0.0 \n",
            "\n",
            "water 1.0 \n",
            "\n",
            "forget 1.0 \n",
            "\n",
            "mention 1.0 \n",
            "\n",
            "spur 1.0 \n",
            "\n",
            "moment 0.5 \n",
            "\n",
            "fourth 0.5 \n",
            "\n",
            "hairy 0.0 \n",
            "\n",
            "wet 1.0 \n",
            "\n",
            "aware 1.0 \n",
            "\n",
            "behold 1.0 \n",
            "\n",
            "secret 0.0 \n",
            "\n",
            "conspirator 1.0 \n",
            "\n",
            "sit_down 1.0 \n",
            "\n",
            "uncanny 1.0 \n",
            "\n",
            "shadow 1.0 \n",
            "\n",
            "noise 1.0 \n",
            "\n",
            "feel_like 1.0 \n",
            "\n",
            "shrink 0.0 \n",
            "\n",
            "out_of_sight 1.0 \n",
            "\n",
            "trap 0.0 \n",
            "\n",
            "dark 0.0 \n",
            "\n",
            "bear_down 0.0 \n",
            "\n",
            "crossing 0.0 \n",
            "\n",
            "saddle 0.0 \n",
            "\n",
            "creep 0.0 \n",
            "\n",
            "voice 1.0 \n",
            "\n",
            "whisper 1.0 \n",
            "\n",
            "take_off 0.0 \n",
            "\n",
            "mistake 1.0 \n",
            "\n",
            "offhand 0.0 \n",
            "\n",
            "skepticism 1.0 \n",
            "\n",
            "frown 1.0 \n",
            "\n",
            "moody 0.0 \n",
            "\n",
            "prevent 0.8571428571428571 \n",
            "\n",
            "sulk 1.0 \n",
            "\n",
            "upset 0.0 \n",
            "\n",
            "appreciation 0.0 \n",
            "\n",
            "uncomfortable 0.0 \n",
            "\n",
            "silence 0.0 \n",
            "\n",
            "subject 1.0 \n",
            "\n",
            "renewed 1.0 \n",
            "\n",
            "listener 1.0 \n",
            "\n",
            "alert 1.0 \n",
            "\n",
            "pause 1.0 \n",
            "\n",
            "moisten 1.0 \n",
            "\n",
            "throat 1.0 \n",
            "\n",
            "break_in 1.0 \n",
            "\n",
            "pay 0.5 \n",
            "\n",
            "bill 0.0 \n",
            "\n",
            "run_around 1.0 \n",
            "\n",
            "hate 1.0 \n",
            "\n",
            "divorce 0.5 \n",
            "\n",
            "scowl 1.0 \n",
            "\n",
            "bitch 0.0 \n",
            "\n",
            "love 0.5 \n",
            "\n",
            "growl 1.0 \n",
            "\n",
            "half 1.0 \n",
            "\n",
            "deal 0.08333333333333333 \n",
            "\n",
            "sell_out 1.0 \n",
            "\n",
            "business 0.6666666666666666 \n",
            "\n",
            "pay_off 0.0 \n",
            "\n",
            "retire 1.0 \n",
            "\n",
            "twinge 1.0 \n",
            "\n",
            "annoyance 0.0 \n",
            "\n",
            "hear 1.0 \n",
            "\n",
            "familiar 1.0 \n",
            "\n",
            "line 0.4 \n",
            "\n",
            "wild 1.0 \n",
            "\n",
            "run 0.3333333333333333 \n",
            "\n",
            "clouded 0.0 \n",
            "\n",
            "suppose 0.5 \n",
            "\n",
            "professional 1.0 \n",
            "\n",
            "nut 0.0 \n",
            "\n",
            "harm 0.0 \n",
            "\n",
            "real_thing 1.0 \n",
            "\n",
            "cunning 0.0 \n",
            "\n",
            "proud 1.0 \n",
            "\n",
            "soft_spot 1.0 \n",
            "\n",
            "offense 1.0 \n",
            "\n",
            "intend 1.0 \n",
            "\n",
            "cop 1.0 \n",
            "\n",
            "suspect 0.0 \n",
            "\n",
            "hook 0.0 \n",
            "\n",
            "alibi 1.0 \n",
            "\n",
            "let 0.6666666666666666 \n",
            "\n",
            "swish 1.0 \n",
            "\n",
            "liquor 1.0 \n",
            "\n",
            "rout 0.0 \n",
            "\n",
            "congressional 1.0 \n",
            "\n",
            "hopeful 0.0 \n",
            "\n",
            "democratic 1.0 \n",
            "\n",
            "presidential 1.0 \n",
            "\n",
            "candidate 1.0 \n",
            "\n",
            "reminder 1.0 \n",
            "\n",
            "federal 1.0 \n",
            "\n",
            "rise 1.0 \n",
            "\n",
            "election 0.8 \n",
            "\n",
            "percent 1.0 \n",
            "\n",
            "choose 0.6666666666666666 \n",
            "\n",
            "different 0.0 \n",
            "\n",
            "party 0.8 \n",
            "\n",
            "percentage 1.0 \n",
            "\n",
            "voter 1.0 \n",
            "\n",
            "tend 1.0 \n",
            "\n",
            "republican 1.0 \n",
            "\n",
            "candidacy 1.0 \n",
            "\n",
            "capture 0.0 \n",
            "\n",
            "popular 0.0 \n",
            "\n",
            "vote 0.6666666666666666 \n",
            "\n",
            "seat 1.0 \n",
            "\n",
            "opposite 0.0 \n",
            "\n",
            "account 0.5 \n",
            "\n",
            "economic 0.3333333333333333 \n",
            "\n",
            "partial 1.0 \n",
            "\n",
            "explanation 1.0 \n",
            "\n",
            "display 1.0 \n",
            "\n",
            "rely 1.0 \n",
            "\n",
            "assumption 0.4 \n",
            "\n",
            "brand 0.3333333333333333 \n",
            "\n",
            "select 1.0 \n",
            "\n",
            "agent 0.0 \n",
            "\n",
            "believe_in 1.0 \n",
            "\n",
            "minimalist 1.0 \n",
            "\n",
            "virtue 1.0 \n",
            "\n",
            "private 0.6666666666666666 \n",
            "\n",
            "market 0.25 \n",
            "\n",
            "vice 1.0 \n",
            "\n",
            "action 0.7058823529411765 \n",
            "\n",
            "government 1.0 \n",
            "\n",
            "intervention 1.0 \n",
            "\n",
            "remedy 1.0 \n",
            "\n",
            "excess 0.0 \n",
            "\n",
            "attendant 0.0 \n",
            "\n",
            "pursuit 0.0 \n",
            "\n",
            "representative 0.2 \n",
            "\n",
            "responsibility 1.0 \n",
            "\n",
            "office 0.25 \n",
            "\n",
            "national 0.3 \n",
            "\n",
            "cast 0.0 \n",
            "\n",
            "legislation 0.0 \n",
            "\n",
            "impose 0.0 \n",
            "\n",
            "confer 0.0 \n",
            "\n",
            "benefit 0.25 \n",
            "\n",
            "population 1.0 \n",
            "\n",
            "attend_to 1.0 \n",
            "\n",
            "willing 1.0 \n",
            "\n",
            "engage 1.0 \n",
            "\n",
            "incentive 1.0 \n",
            "\n",
            "margin 1.0 \n",
            "\n",
            "lean 0.3333333333333333 \n",
            "\n",
            "acquire 1.0 \n",
            "\n",
            "legislator 1.0 \n",
            "\n",
            "powerless 1.0 \n",
            "\n",
            "bring 1.0 \n",
            "\n",
            "respective 1.0 \n",
            "\n",
            "making 1.0 \n",
            "\n",
            "limit 0.5 \n",
            "\n",
            "particular 0.6666666666666666 \n",
            "\n",
            "bring_home 0.0 \n",
            "\n",
            "standing 0.5 \n",
            "\n",
            "enhance 0.0 \n",
            "\n",
            "tax 1.0 \n",
            "\n",
            "advent 1.0 \n",
            "\n",
            "occupy 0.0 \n",
            "\n",
            "society 1.0 \n",
            "\n",
            "confront 0.0 \n",
            "\n",
            "disdain 0.0 \n",
            "\n",
            "constituent 0.0 \n",
            "\n",
            "win 0.0 \n",
            "\n",
            "overall 1.0 \n",
            "\n",
            "associate 1.0 \n",
            "\n",
            "relative 0.3333333333333333 \n",
            "\n",
            "security 0.5 \n",
            "\n",
            "for_example 1.0 \n",
            "\n",
            "vulnerable 1.0 \n",
            "\n",
            "incumbent 1.0 \n",
            "\n",
            "immune 0.0 \n",
            "\n",
            "defeat 1.0 \n",
            "\n",
            "exception 0.0 \n",
            "\n",
            "trend 0.5 \n",
            "\n",
            "freshman 0.0 \n",
            "\n",
            "revolution 1.0 \n",
            "\n",
            "reelection 1.0 \n",
            "\n",
            "emphasis 0.0 \n",
            "\n",
            "partisan 1.0 \n",
            "\n",
            "indicate 0.0 \n",
            "\n",
            "difficult 1.0 \n",
            "\n",
            "viable 1.0 \n",
            "\n",
            "discourage 1.0 \n",
            "\n",
            "prospect 1.0 \n",
            "\n",
            "opponent 0.5 \n",
            "\n",
            "financial 1.0 \n",
            "\n",
            "game 0.038461538461538464 \n",
            "\n",
            "disadvantage 1.0 \n",
            "\n",
            "hinder 0.0 \n",
            "\n",
            "attempt 1.0 \n",
            "\n",
            "competitive 1.0 \n",
            "\n",
            "slate 0.0 \n",
            "\n",
            "fifth 1.0 \n",
            "\n",
            "disproportionate 0.0 \n",
            "\n",
            "power 0.0 \n",
            "\n",
            "translate 0.0 \n",
            "\n",
            "strong 1.0 \n",
            "\n",
            "democrat 0.8333333333333334 \n",
            "\n",
            "fare 0.0 \n",
            "\n",
            "campaign 1.0 \n",
            "\n",
            "counterpart 1.0 \n",
            "\n",
            "empirical 1.0 \n",
            "\n",
            "support 0.0 \n",
            "\n",
            "corroborate 0.0 \n",
            "\n",
            "key 1.0 \n",
            "\n",
            "economist 1.0 \n",
            "\n",
            "senator 1.0 \n",
            "\n",
            "turn_back 0.0 \n",
            "\n",
            "allocate 1.0 \n",
            "\n",
            "personal 0.8333333333333334 \n",
            "\n",
            "staff 1.0 \n",
            "\n",
            "budget 0.6666666666666666 \n",
            "\n",
            "primary 1.0 \n",
            "\n",
            "duty 0.0 \n",
            "\n",
            "valid 1.0 \n",
            "\n",
            "rate 1.0 \n",
            "\n",
            "reflect 1.0 \n",
            "\n",
            "participate 1.0 \n",
            "\n",
            "devote 1.0 \n",
            "\n",
            "datum 1.0 \n",
            "\n",
            "exceed 0.0 \n",
            "\n",
            "proportion 0.3333333333333333 \n",
            "\n",
            "activity 0.3333333333333333 \n",
            "\n",
            "examination 1.0 \n",
            "\n",
            "decision 0.125 \n",
            "\n",
            "reveal 1.0 \n",
            "\n",
            "attribute 1.0 \n",
            "\n",
            "exercise 1.0 \n",
            "\n",
            "status 0.6666666666666666 \n",
            "\n",
            "resource 1.0 \n",
            "\n",
            "committee 1.0 \n",
            "\n",
            "constant 1.0 \n",
            "\n",
            "incumbency 1.0 \n",
            "\n",
            "advantage 0.1111111111111111 \n",
            "\n",
            "regional 0.0 \n",
            "\n",
            "senatorial 1.0 \n",
            "\n",
            "conduct 1.0 \n",
            "\n",
            "positive 0.0 \n",
            "\n",
            "federal_government 1.0 \n",
            "\n",
            "income 1.0 \n",
            "\n",
            "look_on 0.0 \n",
            "\n",
            "prisoner 1.0 \n",
            "\n",
            "dilemma 1.0 \n",
            "\n",
            "split 0.0 \n",
            "\n",
            "investigation 0.0 \n",
            "\n",
            "gubernatorial 1.0 \n",
            "\n",
            "house 0.9090909090909091 \n",
            "\n",
            "sum 0.3333333333333333 \n",
            "\n",
            "part 0.7692307692307693 \n",
            "\n",
            "irrational 1.0 \n",
            "\n",
            "behavior 0.4 \n",
            "\n",
            "rational 1.0 \n",
            "\n",
            "long 1.0 \n",
            "\n",
            "frightening 1.0 \n",
            "\n",
            "devastate 1.0 \n",
            "\n",
            "earthquake 1.0 \n",
            "\n",
            "resident 1.0 \n",
            "\n",
            "comfort 0.0 \n",
            "\n",
            "solace 0.0 \n",
            "\n",
            "screen 0.0 \n",
            "\n",
            "hundred 1.0 \n",
            "\n",
            "computer 1.0 \n",
            "\n",
            "quake 1.0 \n",
            "\n",
            "electronic 1.0 \n",
            "\n",
            "bulletin_board 0.0 \n",
            "\n",
            "link 0.0 \n",
            "\n",
            "vivid 1.0 \n",
            "\n",
            "bulletin 1.0 \n",
            "\n",
            "outpost 0.0 \n",
            "\n",
            "two-thirds 1.0 \n",
            "\n",
            "subscriber 1.0 \n",
            "\n",
            "come_up 0.0 \n",
            "\n",
            "teem 1.0 \n",
            "\n",
            "emotional 0.0 \n",
            "\n",
            "firsthand 1.0 \n",
            "\n",
            "excerpt 1.0 \n",
            "\n",
            "traffic 0.0 \n",
            "\n",
            "nickname 0.0 \n",
            "\n",
            "avenue 0.0 \n",
            "\n",
            "floor 0.5714285714285714 \n",
            "\n",
            "building 1.0 \n",
            "\n",
            "heart 0.0 \n",
            "\n",
            "beat 0.0 \n",
            "\n",
            "ok 0.0 \n",
            "\n",
            "poster 0.0 \n",
            "\n",
            "frame 0.0 \n",
            "\n",
            "file_cabinet 1.0 \n",
            "\n",
            "glance 1.0 \n",
            "\n",
            "guilty 0.0 \n",
            "\n",
            "shake 0.0 \n",
            "\n",
            "toy 1.0 \n",
            "\n",
            "block 0.3333333333333333 \n",
            "\n",
            "toss 0.0 \n",
            "\n",
            "library 0.0 \n",
            "\n",
            "endless 1.0 \n",
            "\n",
            "window 0.2857142857142857 \n",
            "\n",
            "buckle 0.0 \n",
            "\n",
            "shower 0.0 \n",
            "\n",
            "book 0.5 \n",
            "\n",
            "reading_room 1.0 \n",
            "\n",
            "auto 1.0 \n",
            "\n",
            "paint 1.0 \n",
            "\n",
            "shop 0.0 \n",
            "\n",
            "send 1.0 \n",
            "\n",
            "cloud 0.5 \n",
            "\n",
            "black 1.0 \n",
            "\n",
            "smoke 1.0 \n",
            "\n",
            "air 0.75 \n",
            "\n",
            "daughter 1.0 \n",
            "\n",
            "hell 0.0 \n",
            "\n",
            "downtown 1.0 \n",
            "\n",
            "peaceful 1.0 \n",
            "\n",
            "car 1.0 \n",
            "\n",
            "alarm 0.0 \n",
            "\n",
            "go_off 0.0 \n",
            "\n",
            "cat 1.0 \n",
            "\n",
            "nervous 1.0 \n",
            "\n",
            "liquefy 1.0 \n",
            "\n",
            "three 1.0 \n",
            "\n",
            "ground 0.0 \n",
            "\n",
            "red_light 0.0 \n",
            "\n",
            "border 1.0 \n",
            "\n",
            "wave 0.25 \n",
            "\n",
            "pier 0.0 \n",
            "\n",
            "dramatic 0.0 \n",
            "\n",
            "hairline 1.0 \n",
            "\n",
            "crack 0.0 \n",
            "\n",
            "concrete 0.5 \n",
            "\n",
            "slab 1.0 \n",
            "\n",
            "damn 0.0 \n",
            "\n",
            "fishing 1.0 \n",
            "\n",
            "stilt 0.0 \n",
            "\n",
            "brave 0.0 \n",
            "\n",
            "storm 0.0 \n",
            "\n",
            "horrible 1.0 \n",
            "\n",
            "smell 1.0 \n",
            "\n",
            "gas 0.0 \n",
            "\n",
            "bay 1.0 \n",
            "\n",
            "knot 0.0 \n",
            "\n",
            "flame 1.0 \n",
            "\n",
            "hard 1.0 \n",
            "\n",
            "bird 1.0 \n",
            "\n",
            "duck 1.0 \n",
            "\n",
            "swarm 1.0 \n",
            "\n",
            "numb 1.0 \n",
            "\n",
            "town 1.0 \n",
            "\n",
            "phone 1.0 \n",
            "\n",
            "mess 1.0 \n",
            "\n",
            "wine 1.0 \n",
            "\n",
            "quiver 0.0 \n",
            "\n",
            "move 0.5 \n",
            "\n",
            "unpredictable 0.0 \n",
            "\n",
            "interval 1.0 \n",
            "\n",
            "kitchen 1.0 \n",
            "\n",
            "refuge 1.0 \n",
            "\n",
            "desk 1.0 \n",
            "\n",
            "run_out 0.0 \n",
            "\n",
            "distressed 0.0 \n",
            "\n",
            "roll 0.0 \n",
            "\n",
            "brick 1.0 \n",
            "\n",
            "sidewalk 1.0 \n",
            "\n",
            "ooze 1.0 \n",
            "\n",
            "flower 1.0 \n",
            "\n",
            "eerie 0.0 \n",
            "\n",
            "amazing 1.0 \n",
            "\n",
            "heart_rate 1.0 \n",
            "\n",
            "short-term 1.0 \n",
            "\n",
            "memory 0.0 \n",
            "\n",
            "calm 1.0 \n",
            "\n",
            "surreal 0.0 \n",
            "\n",
            "confusion 0.0 \n",
            "\n",
            "quiet_down 1.0 \n",
            "\n",
            "parking_lot 1.0 \n",
            "\n",
            "underground 1.0 \n",
            "\n",
            "center 0.0 \n",
            "\n",
            "trip 1.0 \n",
            "\n",
            "slide 1.0 \n",
            "\n",
            "dog 1.0 \n",
            "\n",
            "someone 1.0 \n",
            "\n",
            "bounce 0.0 \n",
            "\n",
            "scream 1.0 \n",
            "\n",
            "cupboard 1.0 \n",
            "\n",
            "scoot 1.0 \n",
            "\n",
            "doorway 1.0 \n",
            "\n",
            "in_front 1.0 \n",
            "\n",
            "pray 1.0 \n",
            "\n",
            "shiver 0.0 \n",
            "\n",
            "frightened 1.0 \n",
            "\n",
            "cry 0.0 \n",
            "\n",
            "walk_around 1.0 \n",
            "\n",
            "beer 1.0 \n",
            "\n",
            "meet 0.0 \n",
            "\n",
            "client 0.0 \n",
            "\n",
            "flimsy 0.0 \n",
            "\n",
            "build 1.0 \n",
            "\n",
            "rock 1.0 \n",
            "\n",
            "breathe 1.0 \n",
            "\n",
            "lunge 1.0 \n",
            "\n",
            "needless 1.0 \n",
            "\n",
            "session 0.0 \n",
            "\n",
            "escape 0.0 \n",
            "\n",
            "scare 1.0 \n",
            "\n",
            "get_through 0.0 \n",
            "\n",
            "backyard 1.0 \n",
            "\n",
            "lawn 1.0 \n",
            "\n",
            "ocean 1.0 \n",
            "\n",
            "tremor 0.0 \n",
            "\n",
            "throw 1.0 \n",
            "\n",
            "freak 1.0 \n",
            "\n",
            "crystal 0.0 \n",
            "\n",
            "tape 0.0 \n",
            "\n",
            "room 1.0 \n",
            "\n",
            "guess 1.0 \n",
            "\n",
            "relax 1.0 \n",
            "\n",
            "flesh 0.0 \n",
            "\n",
            "total 0.0 \n",
            "\n",
            "flight 0.0 \n",
            "\n",
            "nausea 1.0 \n",
            "\n",
            "commonplace 1.0 \n",
            "\n",
            "symptom 0.0 \n",
            "\n",
            "quiet 0.0 \n",
            "\n",
            "homeless 0.0 \n",
            "\n",
            "bundle 1.0 \n",
            "\n",
            "blue 1.0 \n",
            "\n",
            "sleeping_bag 1.0 \n",
            "\n",
            "sit_up 0.0 \n",
            "\n",
            "great 0.0 \n",
            "\n",
            "agree 1.0 \n",
            "\n",
            "refer 0.3333333333333333 \n",
            "\n",
            "comment 1.0 \n",
            "\n",
            "imply 1.0 \n",
            "\n",
            "statement 0.0 \n",
            "\n",
            "quote 0.5 \n",
            "\n",
            "emphasize 1.0 \n",
            "\n",
            "examine 0.6666666666666666 \n",
            "\n",
            "multitude 0.0 \n",
            "\n",
            "shelter 0.0 \n",
            "\n",
            "necessity 1.0 \n",
            "\n",
            "point_out 1.0 \n",
            "\n",
            "category 1.0 \n",
            "\n",
            "compose 0.0 \n",
            "\n",
            "interaction 1.0 \n",
            "\n",
            "defy 0.0 \n",
            "\n",
            "generalization 0.0 \n",
            "\n",
            "look_to 1.0 \n",
            "\n",
            "possess 1.0 \n",
            "\n",
            "make_up 1.0 \n",
            "\n",
            "exhibit 1.0 \n",
            "\n",
            "create 1.0 \n",
            "\n",
            "connect 0.0 \n",
            "\n",
            "sleep 1.0 \n",
            "\n",
            "rob 1.0 \n",
            "\n",
            "fend 0.0 \n",
            "\n",
            "addiction 0.0 \n",
            "\n",
            "dismiss 1.0 \n",
            "\n",
            "view 0.75 \n",
            "\n",
            "reduction 1.0 \n",
            "\n",
            "assertion 0.0 \n",
            "\n",
            "march 0.5 \n",
            "\n",
            "cite 1.0 \n",
            "\n",
            "insinuate 0.0 \n",
            "\n",
            "crusade 1.0 \n",
            "\n",
            "force 1.0 \n",
            "\n",
            "subscribe 0.0 \n",
            "\n",
            "advertise 0.0 \n",
            "\n",
            "organization 1.0 \n",
            "\n",
            "executive 1.0 \n",
            "\n",
            "deprive 1.0 \n",
            "\n",
            "reader 0.0 \n",
            "\n",
            "write_about 1.0 \n",
            "\n",
            "author 1.0 \n",
            "\n",
            "path 1.0 \n",
            "\n",
            "travel 0.0 \n",
            "\n",
            "contractor 1.0 \n",
            "\n",
            "entrust 1.0 \n",
            "\n",
            "revolve_around 1.0 \n",
            "\n",
            "partner 0.0 \n",
            "\n",
            "programme 1.0 \n",
            "\n",
            "creation 0.5 \n",
            "\n",
            "mandate 0.0 \n",
            "\n",
            "contract 1.0 \n",
            "\n",
            "award 0.0 \n",
            "\n",
            "qualify 1.0 \n",
            "\n",
            "falsify 0.0 \n",
            "\n",
            "ownership 1.0 \n",
            "\n",
            "rebuild 1.0 \n",
            "\n",
            "angle 0.0 \n",
            "\n",
            "sentence 0.0 \n",
            "\n",
            "career 0.0 \n",
            "\n",
            "bribe 1.0 \n",
            "\n",
            "official 1.0 \n",
            "\n",
            "retain 1.0 \n",
            "\n",
            "seek 0.0 \n",
            "\n",
            "receive 0.16666666666666666 \n",
            "\n",
            "assistance 1.0 \n",
            "\n",
            "management 0.0 \n",
            "\n",
            "befall 1.0 \n",
            "\n",
            "fall_short 1.0 \n",
            "\n",
            "ingenuity 1.0 \n",
            "\n",
            "shut_up 1.0 \n",
            "\n",
            "clothes 1.0 \n",
            "\n",
            "wrestle 0.0 \n",
            "\n",
            "arrest 1.0 \n",
            "\n",
            "gloss_over 0.0 \n",
            "\n",
            "scandal 0.0 \n",
            "\n",
            "come_around 1.0 \n",
            "\n",
            "court 1.0 \n",
            "\n",
            "gold 0.0 \n",
            "\n",
            "express 1.0 \n",
            "\n",
            "scoop 0.0 \n",
            "\n",
            "characteristic 1.0 \n",
            "\n",
            "eliminate 1.0 \n",
            "\n",
            "clue 0.0 \n",
            "\n",
            "belong 0.0 \n",
            "\n",
            "redistribute 1.0 \n",
            "\n",
            "wealth 0.0 \n",
            "\n",
            "regulate 1.0 \n",
            "\n",
            "commerce 1.0 \n",
            "\n",
            "maintain 0.0 \n",
            "\n",
            "broker 1.0 \n",
            "\n",
            "pocket 0.0 \n",
            "\n",
            "raise 0.0 \n",
            "\n",
            "reach 0.0 \n",
            "\n",
            "take_up 0.0 \n",
            "\n",
            "represent 0.0 \n",
            "\n",
            "introduce 0.0 \n",
            "\n",
            "embody 0.0 \n",
            "\n",
            "machine 1.0 \n",
            "\n",
            "diner 1.0 \n",
            "\n",
            "scoff 1.0 \n",
            "\n",
            "interrupt 1.0 \n",
            "\n",
            "response 0.0 \n",
            "\n",
            "balloon 0.7142857142857143 \n",
            "\n",
            "take_to 0.0 \n",
            "\n",
            "swell 0.0 \n",
            "\n",
            "estimate 1.0 \n",
            "\n",
            "shape 0.0 \n",
            "\n",
            "resemble 1.0 \n",
            "\n",
            "deny 1.0 \n",
            "\n",
            "attraction 0.0 \n",
            "\n",
            "sign_up 0.0 \n",
            "\n",
            "zip 0.0 \n",
            "\n",
            "ascend 1.0 \n",
            "\n",
            "pilot 1.0 \n",
            "\n",
            "inflate 0.0 \n",
            "\n",
            "basket 0.5 \n",
            "\n",
            "holler 0.0 \n",
            "\n",
            "cow 1.0 \n",
            "\n",
            "amble 1.0 \n",
            "\n",
            "squint 0.0 \n",
            "\n",
            "steer 1.0 \n",
            "\n",
            "heat 1.0 \n",
            "\n",
            "burner 0.0 \n",
            "\n",
            "top 0.3333333333333333 \n",
            "\n",
            "cruise 0.0 \n",
            "\n",
            "hiss 1.0 \n",
            "\n",
            "yell 0.0 \n",
            "\n",
            "plunge 0.0 \n",
            "\n",
            "leap 1.0 \n",
            "\n",
            "wear 1.0 \n",
            "\n",
            "loafer 0.0 \n",
            "\n",
            "lift 1.0 \n",
            "\n",
            "scuttle 1.0 \n",
            "\n",
            "pleasure 0.0 \n",
            "\n",
            "scramble 0.0 \n",
            "\n",
            "enlist 1.0 \n",
            "\n",
            "aid 0.0 \n",
            "\n",
            "farmer 1.0 \n",
            "\n",
            "get_out 0.0 \n",
            "\n",
            "hitch 0.0 \n",
            "\n",
            "pull_out 0.0 \n",
            "\n",
            "rendezvous 0.0 \n",
            "\n",
            "disassemble 1.0 \n",
            "\n",
            "craft 0.0 \n",
            "\n",
            "routine 1.0 \n",
            "\n",
            "yank 1.0 \n",
            "\n",
            "punch 1.0 \n",
            "\n",
            "roll_up 0.0 \n",
            "\n",
            "cram 1.0 \n",
            "\n",
            "duffer 1.0 \n",
            "\n",
            "maul 0.0 \n",
            "\n",
            "tee 1.0 \n",
            "\n",
            "ego 1.0 \n",
            "\n",
            "figure 0.25 \n",
            "\n",
            "clamber 1.0 \n",
            "\n",
            "streak 1.0 \n",
            "\n",
            "resist 0.0 \n",
            "\n",
            "rear 0.0 \n",
            "\n",
            "salute 0.0 \n",
            "\n",
            "plan 1.0 \n",
            "\n",
            "emission 0.0 \n",
            "\n",
            "climate 1.0 \n",
            "\n",
            "posturing 1.0 \n",
            "\n",
            "recrimination 1.0 \n",
            "\n",
            "focus 1.0 \n",
            "\n",
            "friday 1.0 \n",
            "\n",
            "release 0.0 \n",
            "\n",
            "document 0.2 \n",
            "\n",
            "term 0.0 \n",
            "\n",
            "text 1.0 \n",
            "\n",
            "obstacle 1.0 \n",
            "\n",
            "negotiation 1.0 \n",
            "\n",
            "copenhagen 1.0 \n",
            "\n",
            "range 1.0 \n",
            "\n",
            "upper_limit 1.0 \n",
            "\n",
            "temperature 1.0 \n",
            "\n",
            "flexibility 0.0 \n",
            "\n",
            "washington 0.5 \n",
            "\n",
            "outline 0.0 \n",
            "\n",
            "period 0.5 \n",
            "\n",
            "boost 0.0 \n",
            "\n",
            "flood 0.0 \n",
            "\n",
            "drought 1.0 \n",
            "\n",
            "deforestation 1.0 \n",
            "\n",
            "envoy 0.0 \n",
            "\n",
            "language 0.0 \n",
            "\n",
            "cut 0.0 \n",
            "\n",
            "china 1.0 \n",
            "\n",
            "bush_administration 0.0 \n",
            "\n",
            "united_states 0.625 \n",
            "\n",
            "observer 1.0 \n",
            "\n",
            "america 1.0 \n",
            "\n",
            "battle 0.0 \n",
            "\n",
            "economy 1.0 \n",
            "\n",
            "planet 1.0 \n",
            "\n",
            "obligation 1.0 \n",
            "\n",
            "imperative 0.0 \n",
            "\n",
            "delegate 0.0 \n",
            "\n",
            "mexico 1.0 \n",
            "\n",
            "central_america 0.0 \n",
            "\n",
            "treaty 1.0 \n",
            "\n",
            "participant 1.0 \n",
            "\n",
            "draft 0.0 \n",
            "\n",
            "proposal 1.0 \n",
            "\n",
            "push 0.0 \n",
            "\n",
            "pressure 0.0 \n",
            "\n",
            "india 1.0 \n",
            "\n",
            "urgency 1.0 \n",
            "\n",
            "minister 0.0 \n",
            "\n",
            "plane 1.0 \n",
            "\n",
            "hall 1.0 \n",
            "\n",
            "news_conference 1.0 \n",
            "\n",
            "gathering 0.0 \n",
            "\n",
            "activist 1.0 \n",
            "\n",
            "paper 1.0 \n",
            "\n",
            "e-mail 1.0 \n",
            "\n",
            "wednesday 1.0 \n",
            "\n",
            "big_league 1.0 \n",
            "\n",
            "vice_president 1.0 \n",
            "\n",
            "heavyweight 0.0 \n",
            "\n",
            "victory 1.0 \n",
            "\n",
            "israel 1.0 \n",
            "\n",
            "visit 0.5 \n",
            "\n",
            "point 0.0 \n",
            "\n",
            "absence 1.0 \n",
            "\n",
            "success 0.0 \n",
            "\n",
            "sideline 0.0 \n",
            "\n",
            "triumph 1.0 \n",
            "\n",
            "scoreboard 1.0 \n",
            "\n",
            "perimeter 0.0 \n",
            "\n",
            "speculation 0.5 \n",
            "\n",
            "net 0.0 \n",
            "\n",
            "scorer 1.0 \n",
            "\n",
            "quarter 0.0 \n",
            "\n",
            "whistle 0.0 \n",
            "\n",
            "applause 1.0 \n",
            "\n",
            "halftime 1.0 \n",
            "\n",
            "player 1.0 \n",
            "\n",
            "spite 1.0 \n",
            "\n",
            "season 1.0 \n",
            "\n",
            "wall_street 0.0 \n",
            "\n",
            "momentum 1.0 \n",
            "\n",
            "dollar 1.0 \n",
            "\n",
            "heading 0.0 \n",
            "\n",
            "indicator 1.0 \n",
            "\n",
            "closing 0.0 \n",
            "\n",
            "technology 0.0 \n",
            "\n",
            "predominance 1.0 \n",
            "\n",
            "trading 1.0 \n",
            "\n",
            "volume 0.0 \n",
            "\n",
            "publication 0.0 \n",
            "\n",
            "relation 1.0 \n",
            "\n",
            "robustness 1.0 \n",
            "\n",
            "recovery 1.0 \n",
            "\n",
            "analyst 0.3333333333333333 \n",
            "\n",
            "sale 0.6666666666666666 \n",
            "\n",
            "november 1.0 \n",
            "\n",
            "row 0.5 \n",
            "\n",
            "confidence 0.0 \n",
            "\n",
            "consumer 1.0 \n",
            "\n",
            "december 1.0 \n",
            "\n",
            "index 0.0 \n",
            "\n",
            "sector 0.0 \n",
            "\n",
            "treasury 0.0 \n",
            "\n",
            "bond 0.0 \n",
            "\n",
            "yield 0.0 \n",
            "\n",
            "tuesday 1.0 \n",
            "\n",
            "violence 0.0 \n",
            "\n",
            "instability 0.0 \n",
            "\n",
            "infrastructure 0.0 \n",
            "\n",
            "auction 0.0 \n",
            "\n",
            "saturday 1.0 \n",
            "\n",
            "russia 0.0 \n",
            "\n",
            "oil 0.0 \n",
            "\n",
            "firm 1.0 \n",
            "\n",
            "iraq 1.0 \n",
            "\n",
            "war 1.0 \n",
            "\n",
            "sanction 0.0 \n",
            "\n",
            "stake 1.0 \n",
            "\n",
            "bid 0.0 \n",
            "\n",
            "june 1.0 \n",
            "\n",
            "representation 0.0 \n",
            "\n",
            "giant 0.0 \n",
            "\n",
            "editor 1.0 \n",
            "\n",
            "news 1.0 \n",
            "\n",
            "outlet 1.0 \n",
            "\n",
            "long_time 1.0 \n",
            "\n",
            "bombing 0.0 \n",
            "\n",
            "military 0.0 \n",
            "\n",
            "reserve 0.0 \n",
            "\n",
            "rival 0.0 \n",
            "\n",
            "energy 0.0 \n",
            "\n",
            "ministry 0.0 \n",
            "\n",
            "advice 1.0 \n",
            "\n",
            "weekend 1.0 \n",
            "\n",
            "spokesman 1.0 \n",
            "\n",
            "par 0.0 \n",
            "\n",
            "competitor 1.0 \n",
            "\n",
            "barrel 0.0 \n",
            "\n",
            "fee 1.0 \n",
            "\n",
            "output 0.0 \n",
            "\n",
            "bank 1.0 \n",
            "\n",
            "us 1.0 \n",
            "\n",
            "shackle 1.0 \n",
            "\n",
            "protection 0.0 \n",
            "\n",
            "fund 1.0 \n",
            "\n",
            "amount 1.0 \n",
            "\n",
            "broadcaster 0.0 \n",
            "\n",
            "institution 1.0 \n",
            "\n",
            "restriction 1.0 \n",
            "\n",
            "acceptance 0.0 \n",
            "\n",
            "manager 1.0 \n",
            "\n",
            "bonus 0.0 \n",
            "\n",
            "charge 0.0 \n",
            "\n",
            "investor 1.0 \n",
            "\n",
            "loan 1.0 \n",
            "\n",
            "kitty 0.0 \n",
            "\n",
            "height 0.0 \n",
            "\n",
            "investment 1.0 \n",
            "\n",
            "cash 1.0 \n",
            "\n",
            "liability 1.0 \n",
            "\n",
            "capital 1.0 \n",
            "\n",
            "reference 0.0 \n",
            "\n",
            "appeal 1.0 \n",
            "\n",
            "half_a_dozen 1.0 \n",
            "\n",
            "testimony 1.0 \n",
            "\n",
            "defense 0.0 \n",
            "\n",
            "message 0.0 \n",
            "\n",
            "trial 0.0 \n",
            "\n",
            "rape 0.0 \n",
            "\n",
            "bed 1.0 \n",
            "\n",
            "judge 1.0 \n",
            "\n",
            "imposition 1.0 \n",
            "\n",
            "jury 1.0 \n",
            "\n",
            "lawyer 1.0 \n",
            "\n",
            "filing 1.0 \n",
            "\n",
            "argument 0.8 \n",
            "\n",
            "reporting 0.0 \n",
            "\n",
            "reporter 1.0 \n",
            "\n",
            "circus 0.0 \n",
            "\n",
            "atmosphere 1.0 \n",
            "\n",
            "detail 1.0 \n",
            "\n",
            "danger 1.0 \n",
            "\n",
            "inclusion 0.0 \n",
            "\n",
            "trial_judge 1.0 \n",
            "\n",
            "ruling 1.0 \n",
            "\n",
            "last_word 1.0 \n",
            "\n",
            "coverage 1.0 \n",
            "\n",
            "fairness 0.0 \n",
            "\n",
            "conviction 0.0 \n",
            "\n",
            "cleveland 1.0 \n",
            "\n",
            "murder 1.0 \n",
            "\n",
            "carnival 0.0 \n",
            "\n",
            "crime 1.0 \n",
            "\n",
            "inspiration 0.0 \n",
            "\n",
            "movie 1.0 \n",
            "\n",
            "century 1.0 \n",
            "\n",
            "professor 1.0 \n",
            "\n",
            "connecticut 1.0 \n",
            "\n",
            "communication 1.0 \n",
            "\n",
            "execution 1.0 \n",
            "\n",
            "juror 1.0 \n",
            "\n",
            "reaction 0.0 \n",
            "\n",
            "photograph 1.0 \n",
            "\n",
            "victim 1.0 \n",
            "\n",
            "prosecutor 1.0 \n",
            "\n",
            "verdict 1.0 \n",
            "\n",
            "emotion 1.0 \n",
            "\n",
            "crush 0.0 \n",
            "\n",
            "camera 0.0 \n",
            "\n",
            "courthouse 1.0 \n",
            "\n",
            "new_haven 1.0 \n",
            "\n",
            "interview 1.0 \n",
            "\n",
            "journalist 1.0 \n",
            "\n",
            "adaptation 1.0 \n",
            "\n",
            "supreme_court 1.0 \n",
            "\n",
            "sport 0.0 \n",
            "\n",
            "fraud 1.0 \n",
            "\n",
            "law 0.2222222222222222 \n",
            "\n",
            "trick 0.5 \n",
            "\n",
            "will 0.0 \n",
            "\n",
            "expulsion 0.0 \n",
            "\n",
            "ink 1.0 \n",
            "\n",
            "referee 0.0 \n",
            "\n",
            "card 0.0 \n",
            "\n",
            "league 1.0 \n",
            "\n",
            "phase 1.0 \n",
            "\n",
            "discipline 0.0 \n",
            "\n",
            "trainer 1.0 \n",
            "\n",
            "provocation 0.0 \n",
            "\n",
            "football 1.0 \n",
            "\n",
            "ethos 1.0 \n",
            "\n",
            "debate 0.6666666666666666 \n",
            "\n",
            "hymn 1.0 \n",
            "\n",
            "nobility 0.0 \n",
            "\n",
            "honor 0.0 \n",
            "\n",
            "act 1.0 \n",
            "\n",
            "dramatization 0.0 \n",
            "\n",
            "technician 0.0 \n",
            "\n",
            "principle 0.0 \n",
            "\n",
            "regulation 0.0 \n",
            "\n",
            "spirit 0.0 \n",
            "\n",
            "integrity 0.0 \n",
            "\n",
            "aspect 0.0 \n",
            "\n",
            "violation 1.0 \n",
            "\n",
            "punishment 1.0 \n",
            "\n",
            "complaint 0.0 \n",
            "\n",
            "violator 0.0 \n",
            "\n",
            "championship 0.0 \n",
            "\n",
            "participation 1.0 \n",
            "\n",
            "addition 0.0 \n",
            "\n",
            "cycle 0.0 \n",
            "\n",
            "suspension 0.0 \n",
            "\n",
            "application 1.0 \n",
            "\n",
            "controversy 1.0 \n",
            "\n",
            "transmission 1.0 \n",
            "\n",
            "directive 0.0 \n",
            "\n",
            "possibility 1.0 \n",
            "\n",
            "arsenic 0.7692307692307693 \n",
            "\n",
            "nutrient 0.0 \n",
            "\n",
            "scope 1.0 \n",
            "\n",
            "earth 1.0 \n",
            "\n",
            "element 0.0 \n",
            "\n",
            "organism 1.0 \n",
            "\n",
            "capability 0.0 \n",
            "\n",
            "phosphorus 1.0 \n",
            "\n",
            "edition 1.0 \n",
            "\n",
            "journal 0.0 \n",
            "\n",
            "carbon 1.0 \n",
            "\n",
            "hydrogen 1.0 \n",
            "\n",
            "nitrogen 1.0 \n",
            "\n",
            "oxygen 1.0 \n",
            "\n",
            "sulfur 1.0 \n",
            "\n",
            "announcement 0.0 \n",
            "\n",
            "environment 1.0 \n",
            "\n",
            "space 0.0 \n",
            "\n",
            "science 1.0 \n",
            "\n",
            "united_kingdom 1.0 \n",
            "\n",
            "substitution 0.0 \n",
            "\n",
            "hard_time 1.0 \n",
            "\n",
            "machinery 1.0 \n",
            "\n",
            "moon 0.0 \n",
            "\n",
            "democracy 0.0 \n",
            "\n",
            "latin_america 1.0 \n",
            "\n",
            "backing 1.0 \n",
            "\n",
            "unemployment 1.0 \n",
            "\n",
            "brazil 1.0 \n",
            "\n",
            "region 0.6666666666666666 \n",
            "\n",
            "poll 1.0 \n",
            "\n",
            "attitude 1.0 \n",
            "\n",
            "october 1.0 \n",
            "\n",
            "mood 0.0 \n",
            "\n",
            "stability 0.0 \n",
            "\n",
            "south_america 1.0 \n",
            "\n",
            "peru 1.0 \n",
            "\n",
            "drop 0.0 \n",
            "\n",
            "respondent 0.0 \n",
            "\n",
            "chart 1.0 \n",
            "\n",
            "congress 0.5 \n",
            "\n",
            "woe 1.0 \n",
            "\n",
            "recession 0.0 \n",
            "\n",
            "safety_net 1.0 \n",
            "\n",
            "united_nations 1.0 \n",
            "\n",
            "poverty 1.0 \n",
            "\n",
            "popularity 1.0 \n",
            "\n",
            "venezuela 1.0 \n",
            "\n",
            "leadership 0.0 \n",
            "\n",
            "distinction 0.0 \n",
            "\n",
            "dominican_republic 1.0 \n",
            "\n",
            "justice 0.0 \n",
            "\n",
            "dinner 0.4 \n",
            "\n",
            "petition 1.0 \n",
            "\n",
            "review 0.0 \n",
            "\n",
            "overhaul 0.0 \n",
            "\n",
            "rallying_cry 1.0 \n",
            "\n",
            "outcome 0.5 \n",
            "\n",
            "sponsor 1.0 \n",
            "\n",
            "litigation 1.0 \n",
            "\n",
            "hotel 1.0 \n",
            "\n",
            "senate 0.0 \n",
            "\n",
            "guest 1.0 \n",
            "\n",
            "event 1.0 \n",
            "\n",
            "ethic 0.0 \n",
            "\n",
            "code_of_conduct 1.0 \n",
            "\n",
            "code 1.0 \n",
            "\n",
            "canon 1.0 \n",
            "\n",
            "speaker 1.0 \n",
            "\n",
            "proximity 0.0 \n",
            "\n",
            "breach 0.0 \n",
            "\n",
            "indifference 0.0 \n",
            "\n",
            "fundraiser 1.0 \n",
            "\n",
            "attorney 1.0 \n",
            "\n",
            "appearance 0.0 \n",
            "\n",
            "regard 0.0 \n",
            "\n",
            "critic 0.0 \n",
            "\n",
            "conservative 0.0 \n",
            "\n",
            "administration 0.0 \n",
            "\n",
            "trial_court 1.0 \n",
            "\n",
            "indication 1.0 \n",
            "\n",
            "familiarity 1.0 \n",
            "\n",
            "spain 1.0 \n",
            "\n",
            "contrast 0.5 \n",
            "\n",
            "san_jose 0.0 \n",
            "\n",
            "number_one 0.0 \n",
            "\n",
            "selection 0.5 \n",
            "\n",
            "panama 1.0 \n",
            "\n",
            "football_player 1.0 \n",
            "\n",
            "cup 0.0 \n",
            "\n",
            "competition 0.0 \n",
            "\n",
            "shock 0.0 \n",
            "\n",
            "peak 0.0 \n",
            "\n",
            "goalkeeper 0.0 \n",
            "\n",
            "london 1.0 \n",
            "\n",
            "striker 0.0 \n",
            "\n",
            "goal 0.0 \n",
            "\n",
            "immigration 1.0 \n",
            "\n",
            "opportunity 1.0 \n",
            "\n",
            "congestion 1.0 \n",
            "\n",
            "police 1.0 \n",
            "\n",
            "compatibility 0.0 \n",
            "\n",
            "harassment 0.0 \n",
            "\n",
            "france 1.0 \n",
            "\n",
            "bricklayer 1.0 \n",
            "\n",
            "waiter 1.0 \n",
            "\n",
            "viewpoint 1.0 \n",
            "\n",
            "consensus 1.0 \n",
            "\n",
            "left 0.25 \n",
            "\n",
            "tone 0.0 \n",
            "\n",
            "unemployed 0.0 \n",
            "\n",
            "misery 1.0 \n",
            "\n",
            "generosity 1.0 \n",
            "\n",
            "fear 0.5 \n",
            "\n",
            "invasion 0.0 \n",
            "\n",
            "territory 0.0 \n",
            "\n",
            "horde 1.0 \n",
            "\n",
            "plumber 1.0 \n",
            "\n",
            "immigrant 1.0 \n",
            "\n",
            "asset 1.0 \n",
            "\n",
            "migration 1.0 \n",
            "\n",
            "specialist 1.0 \n",
            "\n",
            "wage 1.0 \n",
            "\n",
            "employment 0.6 \n",
            "\n",
            "reality 0.0 \n",
            "\n",
            "replacement 0.0 \n",
            "\n",
            "labor 0.75 \n",
            "\n",
            "trade 1.0 \n",
            "\n",
            "shortage 1.0 \n",
            "\n",
            "surgeon 1.0 \n",
            "\n",
            "complementarity 0.0 \n",
            "\n",
            "arrival 0.0 \n",
            "\n",
            "diversity 1.0 \n",
            "\n",
            "nobel_prize 1.0 \n",
            "\n",
            "winner 1.0 \n",
            "\n",
            "google 1.0 \n",
            "\n",
            "yahoo 0.0 \n",
            "\n",
            "migrant 1.0 \n",
            "\n",
            "contributor 0.0 \n",
            "\n",
            "social_system 1.0 \n",
            "\n",
            "european_country 1.0 \n",
            "\n",
            "organisation 0.0 \n",
            "\n",
            "germany 1.0 \n",
            "\n",
            "contribution 1.0 \n",
            "\n",
            "receipt 0.0 \n",
            "\n",
            "lifetime 1.0 \n",
            "\n",
            "pound 0.0 \n",
            "\n",
            "usa 1.0 \n",
            "\n",
            "uproar 1.0 \n",
            "\n",
            "industry 0.5 \n",
            "\n",
            "dispute 1.0 \n",
            "\n",
            "bankruptcy 1.0 \n",
            "\n",
            "regulator 0.0 \n",
            "\n",
            "string 0.0 \n",
            "\n",
            "promise 1.0 \n",
            "\n",
            "value 0.2 \n",
            "\n",
            "new_york 1.0 \n",
            "\n",
            "settlement 0.0 \n",
            "\n",
            "chairman 1.0 \n",
            "\n",
            "responsiveness 0.0 \n",
            "\n",
            "deposit 0.0 \n",
            "\n",
            "compensation 1.0 \n",
            "\n",
            "billion 0.0 \n",
            "\n",
            "september 1.0 \n",
            "\n",
            "wind 0.0 \n",
            "\n",
            "misconduct 0.0 \n",
            "\n",
            "mortgage 1.0 \n",
            "\n",
            "transaction 1.0 \n",
            "\n",
            "intention 1.0 \n",
            "\n",
            "summary 1.0 \n",
            "\n",
            "european 1.0 \n",
            "\n",
            "assessment 0.5 \n",
            "\n",
            "product 0.0 \n",
            "\n",
            "assess 1.0 \n",
            "\n",
            "recommendation 1.0 \n",
            "\n",
            "information 0.0 \n",
            "\n",
            "condition 0.0 \n",
            "\n",
            "package 1.0 \n",
            "\n",
            "leaflet 0.0 \n",
            "\n",
            "contact 0.5 \n",
            "\n",
            "pharmacist 1.0 \n",
            "\n",
            "discussion 1.0 \n",
            "\n",
            "powder 1.0 \n",
            "\n",
            "solution 1.0 \n",
            "\n",
            "infusion 0.0 \n",
            "\n",
            "drip 0.0 \n",
            "\n",
            "vein 0.0 \n",
            "\n",
            "substance 0.3333333333333333 \n",
            "\n",
            "pleural 1.0 \n",
            "\n",
            "lining 0.0 \n",
            "\n",
            "remove 1.0 \n",
            "\n",
            "surgery 0.0 \n",
            "\n",
            "easily 1.0 \n",
            "\n",
            "not 1.0 \n",
            "\n",
            "advanced 1.0 \n",
            "\n",
            "affect 0.0 \n",
            "\n",
            "obtain 1.0 \n",
            "\n",
            "prescription 0.0 \n",
            "\n",
            "supervision 1.0 \n",
            "\n",
            "dose 1.0 \n",
            "\n",
            "mg 1.0 \n",
            "\n",
            "surface_area 1.0 \n",
            "\n",
            "calculate 1.0 \n",
            "\n",
            "weight 1.0 \n",
            "\n",
            "every 0.0 \n",
            "\n",
            "reduce 1.0 \n",
            "\n",
            "side_effect 0.0 \n",
            "\n",
            "corticosteroid 1.0 \n",
            "\n",
            "vitamin 1.0 \n",
            "\n",
            "injection 0.0 \n",
            "\n",
            "vomit 0.0 \n",
            "\n",
            "fluid 1.0 \n",
            "\n",
            "dehydration 0.0 \n",
            "\n",
            "after 1.0 \n",
            "\n",
            "delay 1.0 \n",
            "\n",
            "discontinue 1.0 \n",
            "\n",
            "blood_count 0.0 \n",
            "\n",
            "abnormal 1.0 \n",
            "\n",
            "recommend 1.0 \n",
            "\n",
            "moderate 1.0 \n",
            "\n",
            "divide 0.0 \n",
            "\n",
            "convert 0.0 \n",
            "\n",
            "enzyme 1.0 \n",
            "\n",
            "building_block 0.0 \n",
            "\n",
            "effectiveness 1.0 \n",
            "\n",
            "survival 1.0 \n",
            "\n",
            "short 1.0 \n",
            "\n",
            "markup 1.0 \n",
            "\n",
            "nowadays 1.0 \n",
            "\n",
            "capable 1.0 \n",
            "\n",
            "operation 0.0 \n",
            "\n",
            "arithmetic 0.0 \n",
            "\n",
            "3d 0.0 \n",
            "\n",
            "graph 1.0 \n",
            "\n",
            "initially 1.0 \n",
            "\n",
            "orient 0.0 \n",
            "\n",
            "consist 0.0 \n",
            "\n",
            "console 0.0 \n",
            "\n",
            "dictionary 1.0 \n",
            "\n",
            "input 0.0 \n",
            "\n",
            "calculation 1.0 \n",
            "\n",
            "basically 1.0 \n",
            "\n",
            "closely 0.0 \n",
            "\n",
            "similar 1.0 \n",
            "\n",
            "subtraction 0.0 \n",
            "\n",
            "multiplication 0.0 \n",
            "\n",
            "root 0.0 \n",
            "\n",
            "specify 0.0 \n",
            "\n",
            "bounded 1.0 \n",
            "\n",
            "variable 0.0 \n",
            "\n",
            "operator 1.0 \n",
            "\n",
            "user 1.0 \n",
            "\n",
            "define 0.0 \n",
            "\n",
            "definition 0.0 \n",
            "\n",
            "conditional 0.0 \n",
            "\n",
            "only_if 1.0 \n",
            "\n",
            "instance 1.0 \n",
            "\n",
            "mainly 1.0 \n",
            "\n",
            "operate 0.0 \n",
            "\n",
            "cos 0.0 \n",
            "\n",
            "sin 0.0 \n",
            "\n",
            "plus 1.0 \n",
            "\n",
            "times 0.0 \n",
            "\n",
            "list 1.0 \n",
            "\n",
            "log 0.0 \n",
            "\n",
            "expression 0.0 \n",
            "\n",
            "load 0.0 \n",
            "\n",
            "execute 1.0 \n",
            "\n",
            "resume 1.0 \n",
            "\n",
            "previous 1.0 \n",
            "\n",
            "reuse 1.0 \n",
            "\n",
            "generate 1.0 \n",
            "\n",
            "fix 1.0 \n",
            "\n",
            "text_editor 1.0 \n",
            "\n",
            "print 0.0 \n",
            "\n",
            "quit 1.0 \n",
            "\n",
            "box 0.0 \n",
            "\n",
            "typical 0.0 \n",
            "\n",
            "mode 0.0 \n",
            "\n",
            "color 1.0 \n",
            "\n",
            "foundation 0.0 \n",
            "\n",
            "organise 0.0 \n",
            "\n",
            "actor 0.0 \n",
            "\n",
            "social 1.0 \n",
            "\n",
            "voluntary 0.0 \n",
            "\n",
            "as_well 1.0 \n",
            "\n",
            "relatively 1.0 \n",
            "\n",
            "planning 1.0 \n",
            "\n",
            "provision 0.0 \n",
            "\n",
            "welfare 1.0 \n",
            "\n",
            "commission 1.0 \n",
            "\n",
            "spanish 1.0 \n",
            "\n",
            "affairs 0.0 \n",
            "\n",
            "exclusion 1.0 \n",
            "\n",
            "comparative 1.0 \n",
            "\n",
            "project 0.0 \n",
            "\n",
            "direct 0.0 \n",
            "\n",
            "profitability 1.0 \n",
            "\n",
            "senior 1.0 \n",
            "\n",
            "fifteen 1.0 \n",
            "\n",
            "industrial 1.0 \n",
            "\n",
            "metal 1.0 \n",
            "\n",
            "banking 0.0 \n",
            "\n",
            "case_study 1.0 \n",
            "\n",
            "portfolio 0.0 \n",
            "\n",
            "initiative 0.0 \n",
            "\n",
            "recruitment 1.0 \n",
            "\n",
            "further 0.0 \n",
            "\n",
            "depth 0.0 \n",
            "\n",
            "analyse 1.0 \n",
            "\n",
            "background 0.5 \n",
            "\n",
            "personnel 1.0 \n",
            "\n",
            "undertake 1.0 \n",
            "\n",
            "synthesise 1.0 \n",
            "\n",
            "casebook 1.0 \n",
            "\n",
            "barrier 0.0 \n",
            "\n",
            "perspective 1.0 \n",
            "\n",
            "alternative 1.0 \n",
            "\n",
            "retirement 1.0 \n",
            "\n",
            "redundancy 0.0 \n",
            "\n",
            "primarily 1.0 \n",
            "\n",
            "retention 1.0 \n",
            "\n",
            "retrain 1.0 \n",
            "\n",
            "gender 0.0 \n",
            "\n",
            "ethnic 1.0 \n",
            "\n",
            "relate 1.0 \n",
            "\n",
            "discrimination 1.0 \n",
            "\n",
            "rural_area 1.0 \n",
            "\n",
            "enterprise 0.0 \n",
            "\n",
            "cover 0.0 \n",
            "\n",
            "united 1.0 \n",
            "\n",
            "kingdom 0.0 \n",
            "\n",
            "veterinary 1.0 \n",
            "\n",
            "available 1.0 \n",
            "\n",
            "tablet 0.0 \n",
            "\n",
            "score 0.0 \n",
            "\n",
            "mark 0.0 \n",
            "\n",
            "quantity 1.0 \n",
            "\n",
            "supportive 1.0 \n",
            "\n",
            "special 1.0 \n",
            "\n",
            "diet 1.0 \n",
            "\n",
            "vomiting 1.0 \n",
            "\n",
            "receptor 1.0 \n",
            "\n",
            "central_nervous_system 1.0 \n",
            "\n",
            "large_number 1.0 \n",
            "\n",
            "carry_out 1.0 \n",
            "\n",
            "placebo 0.0 \n",
            "\n",
            "animal 1.0 \n",
            "\n",
            "intact 1.0 \n",
            "\n",
            "again 1.0 \n",
            "\n",
            "follow-up 1.0 \n",
            "\n",
            "owner 0.0 \n",
            "\n",
            "underlie 1.0 \n",
            "\n",
            "tolerate 0.0 \n",
            "\n",
            "daily 1.0 \n",
            "\n",
            "dosage 0.0 \n",
            "\n",
            "extended 1.0 \n",
            "\n",
            "period_of_time 1.0 \n",
            "\n",
            "maximum 1.0 \n",
            "\n",
            "duration 1.0 \n",
            "\n",
            "precaution 1.0 \n",
            "\n",
            "come_into 1.0 \n",
            "\n",
            "0.6547635461188474\n"
          ]
        }
      ],
      "source": [
        "#AdaBoost\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "words = list(test_dataset.keys())\n",
        "total_accuracy = 0\n",
        "lc = 0\n",
        "i = 0\n",
        "for lemma in words:\n",
        "\n",
        "    if lemma not in dataset:\n",
        "        print(lemma)\n",
        "        continue\n",
        "\n",
        "    X_train = [row[1:-1] for row in dataset[lemma]]\n",
        "    y_train = [row[-1] for row in dataset[lemma]]\n",
        "\n",
        "    X_test = [row[1:-1] for row in test_dataset[lemma]]\n",
        "    y_test = [row[-1] for row in test_dataset[lemma]]\n",
        "\n",
        "    if len(set(y_train)) < 2:\n",
        "        lc += len(y_test)\n",
        "        total_accuracy += 1 * len(X_test)\n",
        "        continue\n",
        "\n",
        "    vectorizer = DictVectorizer(sparse=False)\n",
        "    X_train_vectorized = vectorizer.fit_transform([dict(zip(['lemma', 'sent', 'pos', 'element.text'], x)) for x in X_train])\n",
        "\n",
        "    adaboost_classifier = AdaBoostClassifier(n_estimators=50, random_state=42)  # You can adjust n_estimators as needed\n",
        "\n",
        "    adaboost_classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "    X_test_vectorized = vectorizer.transform([dict(zip(['lemma', 'sent', 'pos', 'element.text'], x)) for x in X_test])\n",
        "\n",
        "    predictions = adaboost_classifier.predict(X_test_vectorized)\n",
        "    # accuracy = adaboost_classifier.score(X_test_vectorized, y_test)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for p, g in zip(predictions, y_test):\n",
        "      total += 1\n",
        "      if p in g:\n",
        "        correct += 1\n",
        "    accuracy = correct/total\n",
        "    # if 1: #list (predictions) != y_test:\n",
        "    #   print(lemma,list(predictions),y_test,accuracy)\n",
        "    #   if list(predictions) != y_test:\n",
        "    #     print (X_train)\n",
        "    #     print (y_train)\n",
        "    #     print (X_test)\n",
        "    #   i += 1\n",
        "    #   if i > 10:\n",
        "    #     break\n",
        "    # print accura\n",
        "    lc += len(X_test)\n",
        "    total_accuracy += accuracy * len(X_test)\n",
        "    print(lemma, accuracy,\"\\n\")\n",
        "\n",
        "print(total_accuracy / lc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "VyVceT2CIIW6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ad1e82-753b-44fb-f1e6-f98542260714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.660140631462843\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Assuming your test_dataset and dataset are defined elsewhere\n",
        "\n",
        "words = list(test_dataset.keys())\n",
        "total_accuracy = 0\n",
        "lc = 0\n",
        "\n",
        "for lemma in words:\n",
        "    if lemma not in dataset:\n",
        "        add_dataset_from_wordnet(lemma)\n",
        "        # print(lemma)\n",
        "        continue\n",
        "\n",
        "    X_train = [row[1:-1] for row in dataset[lemma]]\n",
        "    y_train = [row[-1] for row in dataset[lemma]]\n",
        "\n",
        "    X_test = [row[1:-1] for row in test_dataset[lemma]]\n",
        "    y_test = [row[-1] for row in test_dataset[lemma]]\n",
        "    if len(set(y_train)) < 2:\n",
        "        lc += len(y_test)\n",
        "        total_accuracy += 1 * len(y_test)\n",
        "        continue\n",
        "\n",
        "    vectorizer = DictVectorizer(sparse=False)\n",
        "    X_train_vectorized = vectorizer.fit_transform(\n",
        "        [dict(zip(['lemma', 'sent', 'pos', 'element.text'], x)) for x in X_train]\n",
        "    )\n",
        "\n",
        "    # Initialize Logistic Regression (MaxEnt) classifier\n",
        "    maxent_classifier = LogisticRegression(max_iter=1000)\n",
        "\n",
        "    maxent_classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "    X_test_vectorized = vectorizer.transform(\n",
        "        [dict(zip(['lemma', 'sent', 'pos', 'element.text'], x)) for x in X_test]\n",
        "    )\n",
        "\n",
        "    predictions = maxent_classifier.predict(X_test_vectorized)\n",
        "\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for p, g in zip(predictions, y_test):\n",
        "        total += 1\n",
        "        if p in g:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / total\n",
        "    lc += len(X_test)\n",
        "    total_accuracy += accuracy * len(X_test)\n",
        "\n",
        "print(total_accuracy / lc)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uBJSekY2icc2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}