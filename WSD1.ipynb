{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khush2520/Word-Sense-Disambiguation/blob/main/WSD1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 524,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoX3C52FLSNN",
        "outputId": "0037e88f-3d1a-422b-faec-f432ba9c5f87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "xml_file = 'drive/MyDrive/semcor.data.xml'\n",
        "gold_key = 'drive/MyDrive/semcor.gold.key.txt'\n",
        "test_xml_file = 'drive/MyDrive/ALL.data.xml'\n",
        "test_gold_key = 'drive/MyDrive/ALL.gold.key.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXfXZDkO-RZr",
        "outputId": "f05e039c-fe3f-4bf9-9b95-858fb79714a1"
      },
      "execution_count": 525,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 526,
      "metadata": {
        "id": "DDhgKHonqf1l"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "tree = ET.parse(xml_file)\n",
        "root = tree.getroot()\n",
        "# print(type(root))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 527,
      "metadata": {
        "id": "_uDZSdOr-_CT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a429913d-dadb-4488-e7b4-fbc267a9a0ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "226036\n",
            "33316\n"
          ]
        }
      ],
      "source": [
        "lines = []\n",
        "with open(gold_key, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "Y_values = {}\n",
        "processed_data = []\n",
        "for line in lines:\n",
        "    words = line.strip().split()\n",
        "    processed_data.append(words[1])\n",
        "    Y_values[words[0]] = words[1]\n",
        "#     print(item)\n",
        "Y = processed_data\n",
        "\n",
        "print(len(processed_data))\n",
        "print(len(set(processed_data)))\n",
        "# print(Y_values)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = test_gold_key\n",
        "lines = []\n",
        "with open(test_gold_key, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "# print(lines[0:9])\n",
        "\n",
        "test_Y_values = {}\n",
        "test_processed_data = []\n",
        "for line in lines:\n",
        "    words = line.strip().split()\n",
        "    test_processed_data.append(words[1])\n",
        "    test_Y_values[words[0]] = words[1]\n",
        "    # pos = words[0].find('%')\n",
        "\n",
        "    # processed_data.append(words[0][:pos])\n",
        "\n",
        "test_Y = test_processed_data\n",
        "# print(test_Y_values)\n",
        "print(len(test_processed_data))\n",
        "print(len(set(test_processed_data)))\n",
        "# print(Y_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1Tcd69QGI6V",
        "outputId": "d116683f-9715-48ba-d8f1-1c991fb92e38"
      },
      "execution_count": 528,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7253\n",
            "3520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 529,
      "metadata": {
        "id": "7D6FdXGnui1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dda2fde-bf06-427d-b2c1-78dd8891dfef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20399\n"
          ]
        }
      ],
      "source": [
        "dataset = {}\n",
        "for sentence in root.findall('.//sentence'):\n",
        "    # print(f\"Sentence ID: {sentence.attrib['id']}\")\n",
        "    sent = \"\"\n",
        "    for element in sentence:\n",
        "            sent = sent + \" \" + element.text\n",
        "\n",
        "    for element in sentence:\n",
        "        if element.tag in ['instance']:\n",
        "              lemma = element.attrib.get('lemma')\n",
        "              id = element.attrib.get('id')\n",
        "              pos = element.attrib.get('pos')\n",
        "              if lemma not in dataset:\n",
        "                dataset[lemma] = []\n",
        "              dataset[lemma].append([id,lemma, sent, pos, element.text, ])\n",
        "\n",
        "    # print(sent)\n",
        "\n",
        "print(len(dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for lemma in dataset:\n",
        "  for j in dataset[lemma]:\n",
        "    y = Y_values[j[0]]\n",
        "    pos = y.index('%')\n",
        "    j.append(y[pos:])\n"
      ],
      "metadata": {
        "id": "X9SVPnwrAwjO"
      },
      "execution_count": 530,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_tree = ET.parse(test_xml_file)\n",
        "test_root = test_tree.getroot()\n",
        "# print(type(root))\n",
        "test_dataset = {}\n",
        "for sentence in test_root.findall('.//sentence'):\n",
        "    # print(f\"Sentence ID: {sentence.attrib['id']}\")\n",
        "    sent = \"\"\n",
        "    for element in sentence:\n",
        "            sent = sent + \" \" + element.text\n",
        "    for element in sentence:\n",
        "            if element.tag in ['instance']:\n",
        "              lemma = element.attrib.get('lemma')\n",
        "              id = element.attrib.get('id')\n",
        "              pos = element.attrib.get('pos')\n",
        "              if lemma not in test_dataset:\n",
        "                test_dataset[lemma] = []\n",
        "              test_dataset[lemma].append([id,lemma, sent,pos,element.text])\n",
        "            # print(f\"{element.text} Lemma: {lemma}, POS: {pos}\")\n",
        "    # print(sent)\n",
        "\n",
        "# print(len(test_dataset))\n",
        "# print(features)\n"
      ],
      "metadata": {
        "id": "4KM5ajN4EgPv"
      },
      "execution_count": 531,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for lemma in test_dataset:\n",
        "  for j in test_dataset[lemma]:\n",
        "    y = test_Y_values[j[0]]\n",
        "    pos = y.index('%')\n",
        "    j.append(y[pos:])\n"
      ],
      "metadata": {
        "id": "VobYYLonGDRd"
      },
      "execution_count": 532,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "POS  = {'n':'NOUN', 'a': 'ADJ','s': 'ADJ', 'v': 'VERB', 'r':'ADV'}\n",
        "def add_dataset_from_wordnet(word):\n",
        "    word2 = word.replace('_',' ')\n",
        "    word2 = word2.replace('-',' ')\n",
        "    # print(\"word\", word)\n",
        "    synsets = wn.synsets(word)\n",
        "    if synsets:\n",
        "      for word_synset in synsets:\n",
        "        lemma = word_synset.lemmas()[0].name()\n",
        "        example_sentence = word_synset.examples()[0] if word_synset.examples() else word_synset.definition()\n",
        "\n",
        "        p = word_synset.pos()\n",
        "        pos = POS[p]\n",
        "\n",
        "        # Find the word form in the example sentence\n",
        "        word_form = None\n",
        "        if example_sentence is not None:\n",
        "          word_forms = example_sentence.split()\n",
        "          for wf in word_forms:\n",
        "            wf_synsets = wn.synsets(wf)\n",
        "            for wf_synset in wf_synsets:\n",
        "                wf_lemma = wf_synset.lemmas()[0].name()\n",
        "                # print(wf_l)\n",
        "                if lemma == wf_lemma:\n",
        "                  word_form = wf\n",
        "                  break;\n",
        "            if word_form:\n",
        "              break\n",
        "        for sense in word_synset.lemmas():\n",
        "          sense_key = sense.key()\n",
        "          posi = sense_key.index('%')\n",
        "          if sense_key.startswith(word+'%'):\n",
        "            posi = sense_key.index('%')\n",
        "            break\n",
        "\n",
        "        if word_form is None:\n",
        "          word_form = word\n",
        "        if word not in dataset:\n",
        "          dataset[word] = []\n",
        "        dataset[word].append([None, lemma, example_sentence, pos, word_form, sense_key[posi:]])\n",
        "        # print('hi', [lemma, example_sentence, pos, word_form, sense_key])\n",
        "    else:\n",
        "        print('hi',word, [None])\n",
        "\n"
      ],
      "metadata": {
        "id": "EPWEyepIVZgY"
      },
      "execution_count": 533,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = list(test_dataset.keys())\n",
        "\n",
        "for lemma in words:\n",
        "    add_dataset_from_wordnet(lemma)"
      ],
      "metadata": {
        "id": "Ww4allnlCXkc"
      },
      "execution_count": 534,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = list(test_dataset.keys())\n",
        "total_accuracy = 0\n",
        "lc = 0\n",
        "for lemma in words:\n",
        "    if lemma not in dataset:\n",
        "      print(lemma)\n",
        "      continue\n",
        "\n",
        "    X_train = [row[1:-1] for row in dataset[lemma]]\n",
        "    y_train = [row[-1] for row in dataset[lemma]]\n",
        "\n",
        "\n",
        "    X_test = [row[1:-1] for row in test_dataset[lemma]]\n",
        "    y_test = [row[-1] for row in test_dataset[lemma]]\n",
        "    vectorizer = DictVectorizer(sparse=False)\n",
        "    # X_train_vectorized = vectorizer.fit_transform([dict(zip(['pos', 'element.text'], x)) for x in X_train])\n",
        "    X_train_vectorized = vectorizer.fit_transform([dict(zip(['lemma', 'sent', 'pos', 'element.text'], x)) for x in X_train])\n",
        "\n",
        "    naive_bayes_classifier = MultinomialNB()\n",
        "\n",
        "    naive_bayes_classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "    # X_test_vectorized = vectorizer.transform([dict(zip(['pos', 'element.text'], x)) for x in X_test])\n",
        "    X_test_vectorized = vectorizer.transform([dict(zip(['lemma', 'sent', 'pos', 'element.text'], x)) for x in X_test])\n",
        "    predictions = naive_bayes_classifier.predict(X_test_vectorized)\n",
        "    accuracy = naive_bayes_classifier.score(X_test_vectorized, y_test)\n",
        "\n",
        "    if len(set(y_train)) < 2:\n",
        "        # if list(set(test_y))[0] != list(set(train_y))[0]:\n",
        "        #     print(lemma, test_y, list(set(train_y))[0])\n",
        "\n",
        "        lc += len(y_test)\n",
        "        total_accuracy += 1 * len(X_test)\n",
        "        continue\n",
        "    # print(\"Accuracy:\", accuracy)\n",
        "    lc+= len(X_test)\n",
        "    total_accuracy += accuracy * len(X_test)\n",
        "print(total_accuracy/lc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfjvnilZMyKJ",
        "outputId": "31ed5018-229b-4814-d420-a4de4c02b80f"
      },
      "execution_count": 535,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6321522128774301\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "words = list(test_dataset.keys())\n",
        "total_accuracy = 0\n",
        "lc = 0\n",
        "\n",
        "vectorizer = TfidfVectorizer()  # Initialize vectorizer outside the loop\n",
        "\n",
        "for lemma in words:\n",
        "    if lemma not in dataset:\n",
        "        print(lemma)\n",
        "        continue\n",
        "\n",
        "    train_data = dataset[lemma]\n",
        "    test_data = test_dataset[lemma]\n",
        "\n",
        "    train_features = [row[1:5] for row in train_data]  # Features: lemma, sent, pos, element.text\n",
        "    train_y = [row[5] for row in train_data]  # Target 'y'\n",
        "\n",
        "    test_features = [row[1:5] for row in test_data]\n",
        "    test_y = [row[5] for row in test_data]\n",
        "\n",
        "    train_texts = [' '.join(row) for row in train_features]\n",
        "    test_texts = [' '.join(row) for row in test_features]\n",
        "\n",
        "    train_vectorized = vectorizer.fit_transform(train_texts)\n",
        "    test_vectorized = vectorizer.transform(test_texts)\n",
        "\n",
        "    model = SVC(kernel='linear', gamma='auto')\n",
        "\n",
        "    if len(set(train_y)) < 2:\n",
        "        # if list(set(test_y))[0] != list(set(train_y))[0]:\n",
        "        #     print(lemma, test_y, list(set(train_y))[0])\n",
        "\n",
        "        lc += len(test_texts)\n",
        "        total_accuracy += 1 * len(test_texts)\n",
        "        continue\n",
        "\n",
        "    model.fit(train_vectorized, train_y)\n",
        "    predicted_y = model.predict(test_vectorized)\n",
        "\n",
        "    accuracy = accuracy_score(test_y, predicted_y)\n",
        "    lc += len(test_texts)\n",
        "    total_accuracy += accuracy * len(test_texts)\n",
        "\n",
        "\n",
        "print(total_accuracy / lc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX0oA1EHT4uy",
        "outputId": "b4144e0a-892c-491a-ec96-82375cbd1a0f"
      },
      "execution_count": 536,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6211222942230801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "words = list(test_dataset.keys())\n",
        "total_accuracy = 0\n",
        "lc = 0\n",
        "\n",
        "for lemma in words:\n",
        "    if lemma not in dataset:\n",
        "        print(lemma)\n",
        "        continue\n",
        "\n",
        "    X_train = [row[1:-1] for row in dataset[lemma]]\n",
        "    y_train = [row[-1] for row in dataset[lemma]]\n",
        "\n",
        "    X_test = [row[1:-1] for row in test_dataset[lemma]]\n",
        "    y_test = [row[-1] for row in test_dataset[lemma]]\n",
        "\n",
        "    if len(set(y_train)) < 2:\n",
        "        lc += len(y_test)\n",
        "        total_accuracy += 1 * len(X_test)\n",
        "        continue\n",
        "\n",
        "    vectorizer = DictVectorizer(sparse=False)\n",
        "    X_train_vectorized = vectorizer.fit_transform([dict(zip(['lemma', 'sent', 'pos', 'element.text'], x)) for x in X_train])\n",
        "\n",
        "    knn_classifier = KNeighborsClassifier(n_neighbors=(max(2,len(y_train)//3)))  # You can adjust n_neighbors as needed\n",
        "\n",
        "    knn_classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "    X_test_vectorized = vectorizer.transform([dict(zip(['lemma', 'sent', 'pos', 'element.text'], x)) for x in X_test])\n",
        "\n",
        "    predictions = knn_classifier.predict(X_test_vectorized)\n",
        "    accuracy = knn_classifier.score(X_test_vectorized, y_test)\n",
        "\n",
        "\n",
        "    lc += len(X_test)\n",
        "    total_accuracy += accuracy * len(X_test)\n",
        "\n",
        "print(total_accuracy / lc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nasGEDzg2NzM",
        "outputId": "949305fa-56d3-4a3e-e22c-503d0993da45"
      },
      "execution_count": 538,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6211222942230801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "words = list(test_dataset.keys())\n",
        "total_accuracy = 0\n",
        "lc = 0\n",
        "\n",
        "for lemma in words:\n",
        "    if lemma not in dataset:\n",
        "        print(lemma)\n",
        "        continue\n",
        "\n",
        "    X_train = [row[1:-1] for row in dataset[lemma]]\n",
        "    y_train = [row[-1] for row in dataset[lemma]]\n",
        "\n",
        "    X_test = [row[1:-1] for row in test_dataset[lemma]]\n",
        "    y_test = [row[-1] for row in test_dataset[lemma]]\n",
        "\n",
        "    if len(set(y_train)) < 2:\n",
        "        lc += len(y_test)\n",
        "        total_accuracy += 1 * len(X_test)\n",
        "        continue\n",
        "\n",
        "    vectorizer = DictVectorizer(sparse=False)\n",
        "    X_train_vectorized = vectorizer.fit_transform([dict(zip(['lemma', 'sent', 'pos', 'element.text'], x)) for x in X_train])\n",
        "\n",
        "    adaboost_classifier = AdaBoostClassifier(n_estimators=50, random_state=42)  # You can adjust n_estimators as needed\n",
        "\n",
        "    adaboost_classifier.fit(X_train_vectorized, y_train)\n",
        "\n",
        "    X_test_vectorized = vectorizer.transform([dict(zip(['lemma', 'sent', 'pos', 'element.text'], x)) for x in X_test])\n",
        "\n",
        "    predictions = adaboost_classifier.predict(X_test_vectorized)\n",
        "    accuracy = adaboost_classifier.score(X_test_vectorized, y_test)\n",
        "\n",
        "    lc += len(X_test)\n",
        "    total_accuracy += accuracy * len(X_test)\n",
        "\n",
        "print(total_accuracy / lc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EshXtj34d_u",
        "outputId": "abe04902-30f2-423e-8163-443632a2435d"
      },
      "execution_count": 539,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6347718185578382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iEhU8TDa5-No"
      },
      "execution_count": 539,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}